{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic python data handling analysis modules\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import pickle, os, gc, re\n",
    "# small utility functions\n",
    "from utility import *\n",
    "\n",
    "# interactive jupyter widgets!\n",
    "# https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_directory = \"..\"+os.sep+\"Datasets\"+os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) PERSONALITY DATASET: PERS\n",
    "## (2) SERENDIPTY DATASET: SER\n",
    "## (3) LEARNING DATASET: LEARN\n",
    "## (4) HETREC DATASET: HETREC\n",
    "## (5) ML LATEST DATASET: ML\n",
    "\n",
    "## (6) the-numbers.com DATASET: NUM\n",
    "\n",
    "## (7) themoviedb.org DATASET: TMB (yes it's missing a D, but I'm not changing my scripts now!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) PERSONALITY DATASET\n",
    "\n",
    "# Dataset Citation: Nguyen, T.T., Maxwell Harper, F., Terveen, L. et al. Inf Syst Front (2018) 20: 1173.\n",
    "# https://doi.org/10.1007/s10796-017-9782-y\n",
    "\n",
    "# The personality-data.csv file contains the data about the personalities and the\n",
    "# movie preferences of 1834 users.\n",
    "# The ratings.csv file contains the ratings of users in the personality-data.csv\n",
    "# file contributed.\n",
    "personality_dataset = \"personality-isf2018\"\n",
    "\n",
    "# userid,openness,agreeableness,emotional_stability,conscientiousness,extraversion,assigned metric,\n",
    "# assigned condition,movie_1,predicted_rating_1,movie_2,predicted_rating_2,movie_3,predicted_rating_3,movie_4,\n",
    "# predicted_rating_4,movie_5,predicted_rating_5,movie_6,predicted_rating_6,movie_7,predicted_rating_7,movie_8,\n",
    "# predicted_rating_8,movie_9,predicted_rating_9,movie_10,predicted_rating_10,movie_11,predicted_rating_11,\n",
    "# movie_12,predicted_rating_12,is_personalized,enjoy_watching (1834, 34)\n",
    "PERS_personality_data = pd.read_csv(Dataset_directory+ personality_dataset +os.sep+\"personality-data.csv\")\n",
    "# Userid: the hashed user_id.\n",
    "# Openness: an assessment score (from 1 to 7) assessing user tendency to prefer new experience. 1 means the user has tendency NOT to prefer new experience, 7 means the user has tendency to prefer new experience.\n",
    "# Agreeableness: an assessment score (from 1 to 7) assessing user tendency to be compassionate and cooperative rather than suspicious and antagonistic towards others. 1 means the user has tendency to NOT be compassionate and cooperative. 7 means the user has tendency to be compassionate and cooperative.\n",
    "# Emotional Stability: an assessment score (from 1 to 7) assessing user tendency to have psychological stress. 1 means the user has tendency to have psychological stress, and 7 means the user has tendency to NOT have psychological stress.\n",
    "# Conscientiousness: an assessment score (from 1 to 7) assessing user tendency to be organized and dependable, and show self-discipline. 1 means the user does not have such a tendency, and 7 means the user has such tendency.\n",
    "# Extraversion: an assessment score (from 1 to 7) assessing user tendency to be outgoing. 1 means the user does not have such a tendency, and 7 means the user has such a tendency.\n",
    "\n",
    "# Assigned Metric: one of the follows (serendipity, popularity, diversity, default). Each user, besides being assessed their personality, was evaluated their preferences for a list of 12 movies manipulated with serendipity, popularity, diversity value or none (default option).\n",
    "# Assigned Condition: one of the follows (high, medium, low). Based on the assigned metric, and this assigned condition, the list of movies was generated for the users. For example: if the assigned metric is serendipity and the assigned condition is high, the movies in the list are highly serendipitous. We document how we manipulated the movie list based on the assigned condition and assigned metric in page 6 of our research paper mentioned above.\n",
    "# Movie_x (x is from 1 to 12): The list consists of 12 movies. These fields contain the ids of the twelve movies in the list.\n",
    "# Predicted_rating_x (x is from 1 to 12): the predicted rating of the corresponding movie_x for the user.\n",
    "# Is_Personalized:  The response of the user to the question `This list is personalized for me`. Users answered on the 5-point Likert scale. (1: Strongly Disagree, 5: Strongly Agree).\n",
    "# Enjoy_watching: The response of the user to the question `This list contains movies I think I enjoyed watching`. Users answered on the 5-point Likert scale. (1: Strongly Disagree, 5: Strongly Agree)\n",
    "\n",
    "# userid,movieId,rating,tstamp (1028751, 4)\n",
    "PERS_ratings = pd.read_csv(Dataset_directory+ personality_dataset +os.sep+\"ratings.csv\")\n",
    "# userId: the hashed user_id.\n",
    "# movieId: the id of the movie that the user (corresponding to userId) rated.\n",
    "# rating: the rating (from 0.5 to 5 stars) provided by the user.\n",
    "# tstamp: when the user rated the movie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove random spaces in column names!\n",
    "PERS_personality_data.columns = [x.strip() for x in PERS_personality_data.columns]\n",
    "\n",
    "# remove random spaces and deal with typo in userid\n",
    "PERS_ratings.columns = ['userid', 'movieId', 'rating', 'tstamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERS_ratings -> PERS_movie_ratings\n",
    "PERS_movie_ratings = PERS_ratings[[\"movieId\",\"rating\"]]\\\n",
    "                        .groupby(\"movieId\").apply(lambda x: [x[\"rating\"].count(), x[\"rating\"].mean(), x[\"rating\"].std()])\\\n",
    "                        .apply(pd.Series)\n",
    "PERS_movie_ratings.columns = [\"ratings_n\",\"ratings_mean\",\"ratings_std\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "big5 = ['openness', 'agreeableness', 'emotional_stability',\n",
    "       'conscientiousness', 'extraversion']\n",
    "\n",
    "def get_big5_corr_test(df):\n",
    "    corr_fn = corr_simple_pearsonr\n",
    "    list_of_outputs = {}\n",
    "    if df.shape[0]>1:\n",
    "        for pers in big5:\n",
    "            (list_of_outputs[pers+\"_r\"], list_of_outputs[pers+\"_p\"], list_of_outputs[pers+\"_n\"]) = corr_fn( df[\"rating\"] , df[pers] )\n",
    "            list_of_outputs[pers+\"_mean\"]  = df[pers].mean()\n",
    "            list_of_outputs[pers+\"_std\"]  = df[pers].std()\n",
    "    else:\n",
    "        for pers in big5:\n",
    "            list_of_outputs[pers+\"_r\"] = np.nan\n",
    "            list_of_outputs[pers+\"_p\"] = np.nan\n",
    "            list_of_outputs[pers+\"_n\"] = 1\n",
    "            (list_of_outputs[pers+\"_r\"], list_of_outputs[pers+\"_p\"], list_of_outputs[pers+\"_n\"]) = corr_fn( df[\"rating\"] , df[pers] )\n",
    "            list_of_outputs[pers+\"_mean\"]  = df[pers].mean()\n",
    "            list_of_outputs[pers+\"_std\"]  = df[pers].std()        \n",
    "\n",
    "    return list_of_outputs\n",
    "\n",
    "# 35196 movieIds\n",
    "# 11092 movies with only 1 rating\n",
    "# 20311 movies with only <5 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average big5 rating for each user (they sometimes were sampled more than once)\n",
    "# we're implicitly assuming these are static with noise\n",
    "PERS_userid_by_big5 = PERS_personality_data[['userid']+big5].groupby('userid').mean()\n",
    "\n",
    "# get ratings and add mean-big5 values by the userid\n",
    "PERS_ratings_and_big5 = PERS_ratings[[\"movieId\",\"rating\"]].copy()\n",
    "PERS_ratings_and_big5[big5] = PERS_userid_by_big5.loc[ PERS_ratings[\"userid\"] ].reset_index()[big5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Cook_Tortoise_Imdb_project\\lib\\site-packages\\scipy\\stats\\stats.py:3038: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r_num / r_den\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ~13 mins\n",
    "df = PERS_ratings_and_big5.groupby('movieId').apply(lambda x: get_big5_corr_test(x) ).apply(pd.Series)\n",
    "\n",
    "n_variables = [x for x in df.columns if \"_n\" in x]\n",
    "df[\"N\"] = df[n_variables[0]]\n",
    "df.drop(n_variables, axis=1, inplace=True)\n",
    "df[\"N\"] = df[\"N\"].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT: PERS_ratings_and_big5\n",
    "\n",
    "# <timestamp on ratings/big5 data>\n",
    "# <big5 experiment data - the list they assigned, their predicted ratings, user responses>\n",
    "# we have ~1million ratings for those users ... but maybe ~180,000 are movies on lists they were given as part of the experiment\n",
    "# maybe we should check/separate those?\n",
    "\n",
    "PERS_ratings_and_big5 = df.copy()\n",
    "PERS_ratings_and_big5[PERS_movie_ratings.columns] = PERS_movie_ratings\n",
    "PERS_ratings_and_big5.drop(\"N\",axis=1,inplace=True)\n",
    "PERS_ratings_and_big5[\"movieId\"] = PERS_ratings_and_big5.index\n",
    "PERS_ratings_and_big5.to_csv(Dataset_directory+\"Processed\"+os.sep+\"PERS_ratings_and_big5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del PERS_movie_ratings,PERS_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (2) SERENDIPTY DATASET: SER\n",
    "\n",
    "## (6) serendipity-sac2018\n",
    "# https://grouplens.org/datasets/serendipity-2018/\n",
    "\n",
    "dataset = \"serendipity-sac2018\"\n",
    "\n",
    "# userId,movieId (3840, 2)\n",
    "SER_recommendations = pd.read_csv(Dataset_directory+ dataset +os.sep+\"recommendations.csv\")\n",
    "\n",
    "# userId,movieId,rating,timestamp,predictedRating,s1,s2,s3,s4,s5,s6,s7,s8,q,\n",
    "# s_ser_rel,s_ser_find,s_ser_imp,s_ser_rec,m_ser_rel,m_ser_find,m_ser_imp,m_ser_rec (3840, 2)\n",
    "# survey likert questions about 'serendipitous' (generally unpopular but they like) movies\n",
    "# and then some inferred binary variables based on those answers\n",
    "SER_answers = pd.read_csv(Dataset_directory+ dataset +os.sep+\"answers.csv\")\n",
    "\n",
    "# ORIGINAL CSV HAS PROBLEMS (don't story comma separated list within \"\" ... and then *also* use \"\" for Jed \"nickname\" Jones)\n",
    "# it's only 5-6 entries - MANUAL FIX is fine\n",
    "# movieId,title,releaseDate,directedBy,starring,imdbId,tmdbId,genres (49174, 8)\n",
    "# -> movieId,title,releaseDate,imdbId,tmdbId (good for identifying)\n",
    "# genres -> comma separated string list\n",
    "# starring -> comma separated string list\n",
    "# directedBy -> comma separated string list\n",
    "SER_movies = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movies.csv\") # movieId is unique\n",
    "\n",
    "# userId,movieId,tag,timestamp (628157, 4)\n",
    "SER_tags = pd.read_csv(Dataset_directory+ dataset +os.sep+\"tags.csv\")\n",
    "\n",
    "# userId,movieId,rating,timestamp (9997850, 4)\n",
    "SER_training = pd.read_csv(Dataset_directory+ dataset +os.sep+\"training.csv\")\n",
    "\n",
    "# movieId,tag,relevance (12413640, 3)\n",
    "SER_tag_genome = pd.read_csv(Dataset_directory+ dataset +os.sep+\"tag_genome.csv\")\n",
    "\n",
    "# pick the most relevant tags/tags above certain relevance\n",
    "# SER_tag_genome[ SER_tag_genome[\"relevance\"]>.5 ].groupby('movieId').count()[\"tag\"].hist(bins=100)\n",
    "\n",
    "SER_answers_by_movieId = SER_answers.groupby(\"movieId\").mean().drop([\"userId\",\"timestamp\"],axis=1)\n",
    "SER_answers_by_movieId[\"n\"] = SER_answers.groupby('movieId').count()[\"userId\"]\n",
    "\n",
    "SER_movie_ratings = SER_training[[\"movieId\",\"rating\"]].groupby(\"movieId\").apply(lambda x: [x[\"rating\"].count(), x[\"rating\"].mean(), x[\"rating\"].std()])\n",
    "SER_movie_ratings = SER_movie_ratings.apply(pd.Series)\n",
    "SER_movie_ratings.columns = [\"ratings_n\",\"ratings_mean\",\"ratings_std\"]\n",
    "SER_movie_ratings[\"ratings_n\"] = SER_movie_ratings[\"ratings_n\"].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_directors = SER_movies[\"directedBy\"].apply(lambda x: len(x.split(\",\")) if pd.notnull(x) else np.nan)\n",
    "director_list = SER_movies[\"directedBy\"].apply(lambda x: x.split(\",\") if pd.notnull(x) else np.nan)\n",
    "# (0)1-30 directors, mode=1\n",
    "\n",
    "SER_movies[\"director_list\"] = director_list\n",
    "SER_movies[\"num_directors\"] = num_directors\n",
    "\n",
    "num_genres = SER_movies[\"genres\"].apply(lambda x: len(x.split(\",\")) if pd.notnull(x) else np.nan)\n",
    "genres_list = SER_movies[\"genres\"].apply(lambda x: x.split(\",\") if pd.notnull(x) else np.nan)\n",
    "# (0)1-10 genres, mode =1\n",
    "\n",
    "SER_movies[\"genres_list\"] = genres_list\n",
    "SER_movies[\"num_genres\"] = num_genres\n",
    "\n",
    "num_starring = SER_movies[\"starring\"].apply(lambda x: len(x.split(\",\")) if pd.notnull(x) else np.nan)\n",
    "starring_list = SER_movies[\"starring\"].apply(lambda x: x.split(\",\") if pd.notnull(x) else np.nan)\n",
    "# (0)1-70 starring, mode=5\n",
    "\n",
    "SER_movies[\"starring_list\"] = starring_list\n",
    "SER_movies[\"num_starring\"] = num_starring\n",
    "\n",
    "# titles are form title (year) and not unique\n",
    "\n",
    "# releaseDates = pd.to_datetime(SER_movies[\"releaseDate\"]).apply(lambda x: x.date())\n",
    "# throws error - we'll have to decide where we do all the date/release year processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "SER_genre_set = list(set(flatten(SER_movies[\"genres_list\"].dropna() ) ) )\n",
    "\n",
    "# INT_df[\"SER_genres_list\"] = INT_df[\"SER_genres_list\"].apply(lambda x: eval(x) if pd.notnull(x) else np.nan)\n",
    "flat_list = [item for sublist in SER_movies[\"genres_list\"].dropna().values for item in sublist]\n",
    "genres = pd.DataFrame(flat_list).drop_duplicates()#.sort_values(by=\"movieId\")\n",
    "\n",
    "for gen_ind in genres.index:\n",
    "    gen_name = \"genres__\"+genres.loc[gen_ind,0]\n",
    "    SER_movies[gen_name] = SER_movies[\"genres_list\"].apply(lambda x: genres.loc[gen_ind,0] in x if isinstance(x,list) or pd.notnull(x) else np.nan)\n",
    "\n",
    "SER_movies.drop(\"genres_list\",axis=1,inplace=True)\n",
    "\n",
    "SER_genre_vars = search(SER_movies,\"genres__[A-Z]\",case_sensitive=True).index\n",
    "SER_movies[SER_genre_vars] = SER_movies[SER_genre_vars].replace(True,1).replace(False,0).astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT: SER_movies\n",
    "\n",
    "SER_movies.index = SER_movies[\"movieId\"]\n",
    "SER_movies[SER_movie_ratings.columns] = SER_movie_ratings\n",
    "SER_movies[SER_answers_by_movieId.columns] = SER_answers_by_movieId\n",
    "\n",
    "\n",
    "# tags/tag genome/recommendations - we don't use yet\n",
    "# we only take the 'answers' df because it's really easily to output a per movie average\n",
    "# not finished - some cleaning/prep still required\n",
    "\n",
    "SER_movies.drop([\"directedBy\",\"starring\",\"genres\"],axis=1,inplace=True)\n",
    "SER_movies.to_csv(Dataset_directory+\"Processed\"+os.sep+\"SER_movies\"+\".csv\")\n",
    "\n",
    "# SER_answers_by_movieId.to_csv(Dataset_directory+\"Processed\"+os.sep+\"SER_answers_by_movieId\"+\".csv\")\n",
    "# SER_movie_ratings.to_csv(Dataset_directory+\"Processed\"+os.sep+\"SER_movie_ratings\"+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del SER_movie_ratings,SER_answers_by_movieId, SER_tag_genome, SER_training, person_ids, SER_tags,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (3) LEARNING DATASET: LEARN\n",
    "\n",
    "# https://grouplens.org/datasets/learning-from-sets-of-items-2019/\n",
    "# For this paper: https://www.thinkmind.org/download.php?articleid=eknow_2017_4_10_68011\n",
    "\n",
    "dataset = \"learning-from-sets-2019\"\n",
    "\n",
    "# userId,movieId_1,movieId_2,movieId_3,movieId_4,movieId_5,rating,timestamp (29516, 8)\n",
    "LEARN_set_ratings = pd.read_csv(Dataset_directory+ dataset +os.sep+\"set_ratings.csv\")\n",
    "\n",
    "# userId,movieId,rating,timestamp (458970, 4)\n",
    "LEARN_item_ratings = pd.read_csv(Dataset_directory+ dataset +os.sep+\"item_ratings.csv\")\n",
    "\n",
    "\n",
    "LEARN_movie_ratings = LEARN_item_ratings[[\"movieId\",\"rating\"]].groupby(\"movieId\").apply(lambda x: [x[\"rating\"].count(), x[\"rating\"].mean(), x[\"rating\"].std()])\n",
    "LEARN_movie_ratings = LEARN_movie_ratings.apply(pd.Series)\n",
    "LEARN_movie_ratings.columns = [\"n\",\"ratings_mean\",\"ratings_std\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT: LEARN_movie_ratings\n",
    "\n",
    "# we're not doing anything with the (ratings of sets of movies) set data here, just the accompanying item (one movie-at-a-time data)\n",
    "\n",
    "LEARN_movie_ratings[\"n\"] = LEARN_movie_ratings[\"n\"].astype('int')\n",
    "LEARN_movie_ratings.to_csv(Dataset_directory+\"Processed\"+os.sep+\"LEARN_movie_ratings\"+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del LEARN_movie_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (4) HETREC DATASET: HETREC\n",
    "# http://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-readme.txt\n",
    "# MovieLens + IMDb/Rotten Tomatoes\n",
    "\n",
    "# Some restrictions on commercial use -> Have sent them a begging letter!\n",
    "\n",
    "dataset = \"hetrec2011-movielens-2k-v2\"\n",
    "\n",
    "# This file contains the main actores and actresses of the movies.\n",
    "# A ranking is given to the actors of each movie according to the order in which \n",
    "# they appear on the movie IMDb cast web page. (same as \"order\" in tmdb)\n",
    "# movieID,actorID,actorName,ranking (231742, 4)\n",
    "# movieID *not* unique\n",
    "HETREC_movie_actors = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movie_actors.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# movieID,country (10197, 2)\n",
    "# movieID unique, country only 71 different options, but don't look like iso standard! (\"palestinian occupied territories\")\n",
    "HETREC_movie_countries = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movie_countries.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# movieID,directorID,directorName (10155, 3)\n",
    "# movieID unique, directorName mostly refers to a *single director*\n",
    "# Some weird values = Director Ridley Scott, Daniel Davis [Edward D. Wood Jr.], Grigori Chukhraj & Valentin Yezhov\n",
    "HETREC_movie_directors = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movie_directors.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# movieID,genre (20809, 2)\n",
    "# movieID *not* unique, 20 different genres (some pretty empty short=1, IMAX=25)\n",
    "HETREC_movie_genres = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movie_genres.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# movieID,location1,location2,location3,location4 (49167, 5)\n",
    "# movieID *not* unique, location1-4 operate like address ... a lot of the time location1 is a country\n",
    "# but sometimes it's a ship! or \"Israeli-Jordanian Border\"\n",
    "# Lot of work to get anything more consistent than \"number of locations\"\n",
    "HETREC_movie_locations = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movie_locations.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# movieID,tagID,tagWeight (51795, 3)\n",
    "# powerlaw distribution with tagWeight -could use to really thin down\n",
    "# movieID *not* unique\n",
    "HETREC_movie_tags = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movie_tags.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# id,title,imdbID,spanishTitle,imdbPictureURL,year,rtID,rtAllCriticsRating,rtAllCriticsNumReviews,\n",
    "# rtAllCriticsNumFresh,rtAllCriticsNumRotten,rtAllCriticsScore,rtTopCriticsRating,rtTopCriticsNumReviews,\n",
    "# rtTopCriticsNumFresh,rtTopCriticsNumRotten,rtTopCriticsScore,rtAudienceRating,rtAudienceNumRatings,\n",
    "# rtAudienceScore,rtPictureURL (10197, 21)\n",
    "HETREC_movies = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movies.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# id,value (13222, 2)\n",
    "# id -> content for all tags (1 = \"earth\")\n",
    "HETREC_tags = pd.read_csv(Dataset_directory+ dataset +os.sep+\"tags.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# userID,movieID,rating,date_day,date_month,date_year,date_hour,date_minute,date_second (855598, 9)\n",
    "# full time stamp for each individual rating, broken down into separate columns\n",
    "HETREC_user_ratedmovies = pd.read_csv(Dataset_directory+ dataset +os.sep+\"user_ratedmovies.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# userID,movieID,rating,timestamp (855598, 4)\n",
    "# same but with timestamp object\n",
    "HETREC_user_ratedmovies_timestamps = pd.read_csv(Dataset_directory+ dataset +os.sep+\"user_ratedmovies-timestamps.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# userID,movieID,tagID,date_day,date_month,date_year,date_hour,date_minute,date_second (47957, 9)\n",
    "# full time stamp for each individual tagging, broken down into separate columns\n",
    "HETREC_user_taggedmovies = pd.read_csv(Dataset_directory+ dataset +os.sep+\"user_taggedmovies.dat\", sep=r'\\t', engine='python')\n",
    "\n",
    "# userID,movieID,tagID,timestamp (47957, 4)\n",
    "# same but with timestamp object\n",
    "HETREC_user_taggedmovies_timestamps = pd.read_csv(Dataset_directory+ dataset +os.sep+\"user_taggedmovies-timestamps.dat\", sep=r'\\t', engine='python')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HETREC_user_ratedmovies_timestamps.groupby(\"movieId\")\n",
    "HETREC_user_ratedmovies_timestamps = HETREC_user_ratedmovies_timestamps.rename(columns = {'movieID':'movieId'})\n",
    "HETREC_movie_ratings = HETREC_user_ratedmovies_timestamps[[\"movieId\",\"rating\"]].groupby(\"movieId\").apply(lambda x: [x[\"rating\"].count(), x[\"rating\"].mean(), x[\"rating\"].std()])\n",
    "# ML_movie_ratings = ML_ratings[[\"movieId\",\"rating\"]].groupby(\"movieId\").apply(lambda x: [x[\"rating\"].count(), x[\"rating\"].mean(), x[\"rating\"].std()])\n",
    "HETREC_movie_ratings = HETREC_movie_ratings.apply(pd.Series)\n",
    "HETREC_movie_ratings.columns = [\"ratings_n\",\"ratings_mean\",\"ratings_std\"]\n",
    "HETREC_movie_ratings['ratings_n'] = HETREC_movie_ratings['ratings_n'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonise imdbId column to match all others\n",
    "HETREC_movies = HETREC_movies.rename(columns = {'imdbID':'imdbId'})\n",
    "\n",
    "# harmonise id columns to movieID and set index equal to that\n",
    "HETREC_movies = HETREC_movies.rename(columns = {'id':'movieId'})\n",
    "HETREC_movies.index = HETREC_movies[\"movieId\"]\n",
    "\n",
    "# add country data to HETREC_movies\n",
    "HETREC_movie_countries = HETREC_movie_countries.rename(columns = {'movieID':'movieId'})\n",
    "HETREC_movie_countries.index = HETREC_movie_countries[\"movieId\"]\n",
    "HETREC_movies[\"country\"] = HETREC_movie_countries[\"country\"]\n",
    "\n",
    "# add director data to HETREC_movies\n",
    "HETREC_movie_directors = HETREC_movie_directors.rename(columns = {'movieID':'movieId'})\n",
    "HETREC_movie_directors.index = HETREC_movie_directors[\"movieId\"]\n",
    "HETREC_movies[[\"directorID\",\"directorName\"]] = HETREC_movie_directors[[\"directorID\",\"directorName\"]]\n",
    "\n",
    "# add movie_ratings to HETREC_movies\n",
    "HETREC_movies[HETREC_movie_ratings.columns] = HETREC_movie_ratings\n",
    "\n",
    "# imdbPictureURL 181 nulls\n",
    "# rtID 311 nulls\n",
    "# id unique\n",
    "# imdbID -> not unique\n",
    "# rtID -> not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['rtAllCriticsRating', 'rtAllCriticsNumReviews',\n",
    "       'rtAllCriticsNumFresh', 'rtAllCriticsNumRotten', 'rtAllCriticsScore',\n",
    "       'rtTopCriticsRating', 'rtTopCriticsNumReviews', 'rtTopCriticsNumFresh',\n",
    "       'rtTopCriticsNumRotten', 'rtTopCriticsScore', 'rtAudienceRating',\n",
    "       'rtAudienceNumRatings', 'rtAudienceScore']\n",
    "HETREC_movies[numerical_cols] = HETREC_movies[numerical_cols].replace(\"\\\\N\",np.nan).apply(lambda x: pd.to_numeric(x))\n",
    "\n",
    "# \"\\\\N\" appears to be the code for nan (maybe in same 230??)\n",
    "\n",
    "# imdbPictureURL -> full url for jpg, not unique\n",
    "# rtPictureURL -> \\\\N for 230, post_default for 57, otherwise similar non-unique pattern\n",
    "\n",
    "# rtId -> string id, similar-but-not-identical pattern of not-uniqueness\n",
    "# rtAllCriticsRating -> 0 is very common, \\\\N also present (0.0-9.5999)\n",
    "# rtAllCriticsNumReviews -> integers\n",
    "# rtAllCriticsNumFresh -> integers how many gave a \"fresh\" rating\n",
    "# rtAllCriticsNumRotten -> integers how many gave a \"rotten\" rating (skewed v against!)\n",
    "# rtTopCriticsScore -> integer score 0-100 (lot of 0s!)\n",
    "# rtAudienceRating -> float 0.0-5.0 (lot of 0s!)\n",
    "# rtAudienceNumRatings-> integers lotof 0s!\n",
    "# rtAudienceScore -> integeers 0-100, lot of 0s!\n",
    "# HETREC_movies[\"rtAudienceNumRatings\"].value_counts()\n",
    "# HETREC_movies[\"rtAudienceRating\"].replace(\"\\\\N\",np.nan).astype('float').max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotten tomatoes data, adding some extra variables and setting missing values appropriately\n",
    "\n",
    "rtAllCriticsNoReviews = HETREC_movies[\"rtAllCriticsNumReviews\"]==0.0\n",
    "HETREC_movies[\"rtAllCriticsFracRotten\"] = HETREC_movies[\"rtAllCriticsNumRotten\"]/HETREC_movies[\"rtAllCriticsNumReviews\"]\n",
    "HETREC_movies.loc[rtAllCriticsNoReviews,\"rtAllCriticsRating\"] = np.nan\n",
    "HETREC_movies.loc[rtAllCriticsNoReviews,\"rtAllCriticsScore\"] = np.nan\n",
    "HETREC_movies.loc[rtAllCriticsNoReviews,\"rtAllCriticsFracRotten\"] = np.nan\n",
    "\n",
    "rtTopCriticsNoReviews = HETREC_movies[\"rtTopCriticsNumReviews\"]==0.0\n",
    "HETREC_movies[\"rtTopCriticsFracRotten\"] = HETREC_movies[\"rtTopCriticsNumRotten\"]/HETREC_movies[\"rtTopCriticsNumReviews\"]\n",
    "HETREC_movies.loc[rtTopCriticsNoReviews,\"rtTopCriticsRating\"] = np.nan\n",
    "HETREC_movies.loc[rtTopCriticsNoReviews,\"rtTopCriticsScore\"] = np.nan\n",
    "HETREC_movies.loc[rtTopCriticsNoReviews,\"rtTopCriticsFracRotten\"] = np.nan\n",
    "\n",
    "rtAudienceNoRatings = HETREC_movies[\"rtAudienceNumRatings\"]==0.0\n",
    "# HETREC_movies[\"rtTopCriticsFracRotten\"] = HETREC_movies[\"rtTopCriticsNumRotten\"]/HETREC_movies[\"rtTopCriticsNumReviews\"]\n",
    "HETREC_movies.loc[rtAudienceNoRatings,\"rtAudienceRating\"] = np.nan\n",
    "HETREC_movies.loc[rtAudienceNoRatings,\"rtAudienceScore\"] = np.nan\n",
    "# HETREC_movies[\"rtTopCriticsFracRotten\"][rtAudienceNoRatings] = np.nan\n",
    "\n",
    "HETREC_movies.loc[ HETREC_movies[\"rtAllCriticsRating\"]==0.0 , \"rtAllCriticsRating\"] = np.nan\n",
    "HETREC_movies.loc[ HETREC_movies[\"rtTopCriticsRating\"]==0.0 , \"rtTopCriticsRating\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add genre data to HETREC_movies\n",
    "genres = pd.DataFrame(HETREC_movie_genres['genre'].unique())\n",
    "\n",
    "for gen_ind in genres.index:\n",
    "    gen_name = \"genres__\"+genres.loc[gen_ind,0]    \n",
    "    genre_indices = HETREC_movie_genres[ HETREC_movie_genres[\"genre\"]==genres.loc[gen_ind,0] ][\"movieID\"]\n",
    "    HETREC_movies[gen_name] = np.nan\n",
    "    \n",
    "    HETREC_movies.loc[HETREC_movie_genres[\"movieID\"].unique(),gen_name] = 0\n",
    "    HETREC_movies.loc[genre_indices,gen_name] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT: HETREC_movies, HETREC_movie_actors\n",
    "# output per movie dataset\n",
    "HETREC_movies.to_csv(Dataset_directory+\"Processed\"+os.sep+\"HETREC_movies\"+\".csv\")\n",
    "\n",
    "# we're not using HETREC_movie_locations because it looks like junk/a nightmare to extract something useful from\n",
    "\n",
    "# HETREC_tags/HETREC_user_taggedmovies/HETREC_user_taggedmovies_timestamps can wait until we try do something with tags\n",
    "\n",
    "# this will be work to integrate - we'll have to check whether it's worth it (overlap with tmbd_cast?)\n",
    "HETREC_movie_actors[\"order\"] = HETREC_movie_actors[\"ranking\"]-1\n",
    "# ranking same as tmdb \"order\" but starts at 1 not 0\n",
    "HETREC_movie_actors.drop(\"ranking\",axis=1,inplace=True)\n",
    "HETREC_movie_actors.to_csv(Dataset_directory+\"Processed\"+os.sep+\"HETREC_cast\"+\".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del HETREC_movie_actors,HETREC_movie_directors,\n",
    "# del HETREC_user_taggedmovies_timestamps,HETREC_user_taggedmovies,HETREC_user_ratedmovies_timestamps,\n",
    "# del HETREC_user_ratedmovies,HETREC_tags,HETREC_movie_locations,HETREC_movie_tags,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (5) ML LATEST DATASET: ML\n",
    "## The Movie Lens latest 58k movies\n",
    "\n",
    "dataset = \"Movie_Lens_Latest\"\n",
    "# informative README.txt!\n",
    "\n",
    "# tagId,tag (1128,  2)\n",
    "# id -> id content\n",
    "ML_genome_tags = pd.read_csv(Dataset_directory+ dataset +os.sep+\"genome-tags.csv\")\n",
    "\n",
    "# movieId,imdbId,tmdbId (58,098,  3)\n",
    "ML_links = pd.read_csv(Dataset_directory+ dataset +os.sep+\"links.csv\")\n",
    "\n",
    "# movieId,title,genres (58,098,  3)\n",
    "# genres -> | separated list\n",
    "#    Romance,Fantasy,IMAX,Action,Sci-Fi,Western,Drama,Horror,Mystery,Adventure,War,Comedy,Musical,Documentary,Thriller,\n",
    "#    Film-Noir,(no genres listed),Crime,Animation,Children'\n",
    "ML_movies = pd.read_csv(Dataset_directory+ dataset +os.sep+\"movies.csv\")\n",
    "\n",
    "# userId,movieId,tag,timestamp (1,108,997,  4)\n",
    "# tags look user submitted - 74714 different tags - not consistent with genome_tags length/tagId range\n",
    "ML_tags = pd.read_csv(Dataset_directory+ dataset +os.sep+\"tags.csv\")\n",
    "\n",
    "# movieId,tagId,relevance (14,862,528,  3)\n",
    "# each of 1128 tagIds x 13176 movieIds (dense matrix)\n",
    "# relevance is machine learning output (inverse distribution)\n",
    "ML_genome_scores = pd.read_csv(Dataset_directory+ dataset +os.sep+\"genome-scores.csv\")\n",
    "\n",
    "# userId,movieId,rating,timestamp (27,753,444,  4)\n",
    "ML_ratings = pd.read_csv(Dataset_directory+ dataset +os.sep+\"ratings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_movie_ratings = ML_ratings[[\"movieId\",\"rating\"]].groupby(\"movieId\").apply(lambda x: [x[\"rating\"].count(), x[\"rating\"].mean(), x[\"rating\"].std()])\n",
    "ML_movie_ratings = ML_movie_ratings.apply(pd.Series)\n",
    "ML_movie_ratings.columns = [\"ratings_n\",\"ratings_mean\",\"ratings_std\"]\n",
    "ML_movie_ratings['ratings_n'] = ML_movie_ratings['ratings_n'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_movies[\"genres_list\"] = ML_movies[\"genres\"].apply(lambda x: x.split(\"|\") if pd.notnull(x) else np.nan)\n",
    "\n",
    "flat_list = [item for sublist in ML_movies[\"genres_list\"].dropna().values for item in sublist]\n",
    "genres = pd.DataFrame(flat_list).drop_duplicates()#.sort_values(by=\"movieId\")\n",
    "drop_ind = genres[genres[0]==\"(no genres listed)\"].index[0]\n",
    "genres.drop(drop_ind , inplace=True)\n",
    "for gen_ind in genres.index: # ditch \"no genre\" index!\n",
    "    gen_name = \"genres__\"+genres.loc[gen_ind,0] \n",
    "    ML_movies[gen_name] = ML_movies[\"genres_list\"].apply(lambda x: genres.loc[gen_ind,0] in x if isinstance(x,list) or pd.notnull(x) else np.nan).astype('float')\n",
    "    \n",
    "# ML_genre_vars = search(ML_movies,\"ML_genres__[A-Z]\",case_sensitive=True)\n",
    "# ML_genre_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT: ML_movies, ML_links\n",
    "\n",
    "\n",
    "# deal with tags/relevance scores later\n",
    "\n",
    "ML_movies.index = ML_movies[\"movieId\"]\n",
    "ML_links.index = ML_links[\"movieId\"]\n",
    "ML_movies[ML_movie_ratings.columns] = ML_movie_ratings\n",
    "ML_movies.drop([\"genres\",\"genres_list\"],axis=1,inplace=True)\n",
    "ML_movies[ML_links.columns] = ML_links\n",
    "\n",
    "\n",
    "ML_movies.to_csv(Dataset_directory+\"Processed\"+os.sep+\"ML_movies.csv\")\n",
    "\n",
    "# ML_movie_ratings.to_csv(Dataset_directory+\"Processed\"+os.sep+\"ML_movie_ratings.csv\") \n",
    "# ML_links.index = ML_links[\"movieId\"]\n",
    "# ML_links.to_csv(Dataset_directory+\"Processed\"+os.sep+\"ML_links.csv\")\n",
    "# could split up genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del ML_movie_ratings,ML_genome_tags,ML_tags,ML_genome_scores,ML_ratings,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (6) the-numbers.com DATASET: NUM\n",
    "\n",
    "# superficial, polite, respectful scrape of freely available financial data from the-numbers.com\n",
    "# 5769 films\n",
    "# title|link|production_budget|domestic_gross|worldwide_gross|release_year|release_month|release_day|movieId|worldwide_gross_divided_by_budget|international_gross|domestic_gross_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT: NUM_movies\n",
    "\n",
    "NUM_movies = pd.read_csv(Dataset_directory+\"Processed\"+os.sep+\"NUM_movies\"+\".csv\", index_col = \"scrape_count.1\")\n",
    "NUM_movies.index.name = \"scrape_count\"\n",
    "NUM_movies.drop( [\"scrape_count\",\"date\",\"page_no\"], axis=1, inplace=True )\n",
    "\n",
    "NUM_movies[ ['production_budget', 'domestic_gross', 'worldwide_gross'] ].replace(0, np.nan, inplace=True)\n",
    "\n",
    "NUM_movies[\"worldwide_gross_divided_by_budget\"] = NUM_movies[\"worldwide_gross\"] / NUM_movies[\"production_budget\"]\n",
    "NUM_movies[\"international_gross\"] = (NUM_movies[\"worldwide_gross\"] - NUM_movies[\"domestic_gross\"])\n",
    "NUM_movies[\"domestic_gross_fraction\"] = (NUM_movies[\"domestic_gross\"] / NUM_movies[\"worldwide_gross\"])\n",
    "\n",
    "NUM_movies.to_msgpack(Dataset_directory+\"Processed\"+os.sep+\"NUM_movies\"+\".msgpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (7) themoviedatabase.com API DATASET: TMB\n",
    "\n",
    "tmdb_dir = create_subdir(Dataset_directory, \"themoviedb\")\n",
    "\n",
    "\n",
    "company = pd.read_msgpack(tmdb_dir + \"company_cleaned.msgpack\")\n",
    "collections = pd.read_msgpack(tmdb_dir + \"collections_cleaned.msgpack\")\n",
    "person_ids = pd.read_msgpack(tmdb_dir + \"person_ids_cleaned.msgpack\")\n",
    "person_ids.index = person_ids.id\n",
    "\n",
    "keywords_categories = pd.read_msgpack(tmdb_dir + \"keywords_cleaned.msgpack\")\n",
    "keywords_categories.index = keywords_categories.id\n",
    "genre_categories = pd.read_msgpack( tmdb_dir + \"genres.msgpack\")\n",
    "genre_categories.index = genre_categories.id\n",
    "production_country_categories = pd.read_msgpack( tmdb_dir + \"production_countries.msgpack\")\n",
    "language_categories = pd.read_msgpack( tmdb_dir + \"languages.msgpack\") ## list of tmdb spoken languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'adult|alternative_titles|backdrop_path|belongs_to_collection|budget|genres|homepage|tmbdId|imdbId|keywords|\n",
    "# original_language|original_title|overview|popularity|poster_path|production_companies|production_countries|\n",
    "# release_date|revenue|runtime|spoken_languages|status|tagline|title|video|vote_average|vote_count|\n",
    "# facebook_id|instagram_id|twitter_id|num_genres|num_production_companies|num_production_countries|num_spoken_languages|\n",
    "# num_alternative_titles|num_keywords|cast_size|crew_size|cast_credit_list|crew_credit_list'\n",
    "movies = pd.read_msgpack(tmdb_dir + \"movies_full_final.msgpack\")\n",
    "movies.rename(columns = {'id':'tmdbId'},inplace=True)\n",
    "movies.rename(columns = {'imdb_id':'imdbId'},inplace=True)\n",
    "movies.index.name = 'tmdbId'\n",
    "\n",
    "# 'cast_id|character|credit_id|gender|id|order|tmdb_id|max_order|fractional_order'\n",
    "TMB_cast = pd.read_msgpack( tmdb_dir+\"TMB_cast\"+\".msgpack\")\n",
    "# 'credit_id|department|gender|id|job|tmdb_id|order|max_order|fractional_order'\n",
    "TMB_crew = pd.read_msgpack( tmdb_dir+\"TMB_crew\"+\".msgpack\")\n",
    "\n",
    "\n",
    "# 'gender|id|name|profile_path|credit_list|credit_number|mean_order|fractional_mean_order|order_list|gender_guesser|\n",
    "# importulence|profile_path_cast|gender_guess_integrated|cast_crew_overlap|adult|known_for_department|popularity|\n",
    "# death_day|death_month|death_year|birth_day|birth_month|birth_year'\n",
    "TMB_cast_individuals = pd.read_msgpack( tmdb_dir+\"TMB_cast_individuals\"+\".msgpack\")\n",
    "\n",
    "# 'gender|id|name|profile_path|credit_list|credit_number|mean_order|fractional_mean_order|\n",
    "# order_list|department_list|department_number|job_list|job_number|gender_guesser|importulence|\n",
    "# gender_guess_integrated|cast_crew_overlap|adult|known_for_department|popularity|\n",
    "# death_day|death_month|death_year|birth_day|birth_month|birth_year'\n",
    "TMB_crew_individuals = pd.read_msgpack( tmdb_dir+\"TMB_crew_individuals\"+\".msgpack\")\n",
    "\n",
    "# index tmdb\n",
    "# 'gender_guess_integrated_crew_mean|gender_guess_integrated_cast_mean|gender_guess_integrated_crew_lead|\n",
    "# gender_guess_integrated_cast_lead|gender_guess_integrated_crew_lead5|gender_guess_integrated_cast_lead5|\n",
    "# credit_number_crew_mean|credit_number_cast_mean|credit_number_crew_lead|credit_number_cast_lead|credit_number_crew_lead5|\n",
    "# credit_number_cast_lead5|job_number_crew_mean|job_number_crew_lead|job_number_crew_lead5|department_number_crew_mean|\n",
    "# department_number_crew_lead|department_number_crew_lead5|mean_order_crew_mean|mean_order_cast_mean|mean_order_crew_lead|\n",
    "# mean_order_cast_lead|mean_order_crew_lead5|mean_order_cast_lead5|fractional_mean_order_crew_mean|\n",
    "# fractional_mean_order_cast_mean|fractional_mean_order_crew_lead|fractional_mean_order_cast_lead|\n",
    "# fractional_mean_order_crew_lead5|fractional_mean_order_cast_lead5|importulence_crew_mean|importulence_cast_mean|\n",
    "# importulence_crew_lead|importulence_cast_lead|importulence_crew_lead5|importulence_cast_lead5'\n",
    "TMB_cast_crew_aggregates = pd.read_msgpack(tmdb_dir+\"TMB_cast_crew_aggregates\"+\".msgpack\")\n",
    "TMB_cast_crew_aggregates.index.name = \"tmdbId\"\n",
    "\n",
    "\n",
    "# very chunky!\n",
    "# release_dates = pd.read_msgpack(tmdb_dir + \"release_dates.msgpack\")\n",
    "movies[TMB_cast_crew_aggregates.columns]= TMB_cast_crew_aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'INT_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-f03b9efcb4dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mTMB_genre_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"genres__[A-Z]\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcase_sensitive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmovies\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mTMB_genre_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mINT_df\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mTMB_genre_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# TMB_genre_vars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'INT_df' is not defined"
     ]
    }
   ],
   "source": [
    "# SER_genre_set = list(set(flatten(INT_df[\"SER_genres_list\"].dropna() ) ) )\n",
    "TMB_genre_set = list(genre_categories[\"name\"].values)\n",
    "# INT_df[\"SER_genres_list\"] = INT_df[\"SER_genres_list\"].apply(lambda x: eval(x) if pd.notnull(x) else np.nan)\n",
    "# flat_list = [item for sublist in INT_df[\"TMB_genres\"].dropna().values for item in sublist]\n",
    "genres = pd.DataFrame(TMB_genre_set).drop_duplicates()#.sort_values(by=\"movieId\")\n",
    "\n",
    "for gen_ind in genres.index:\n",
    "    gen_name = \"genres__\"+genres.loc[gen_ind,0]\n",
    "    movies[gen_name] = movies[\"genres\"].apply(lambda x: genre_categories.index[gen_ind] in x if isinstance(x,list) or pd.notnull(x) else np.nan)\n",
    "\n",
    "\n",
    "movies.drop( [\"genres\"] , axis=1, inplace=True)    \n",
    "TMB_genre_vars = search(movies,\"genres__[A-Z]\",case_sensitive=True)\n",
    "movies[ TMB_genre_vars.index ] = movies[ TMB_genre_vars.index].replace(True,1).replace(False,0).astype('float32')    \n",
    "# TMB_genre_vars\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adult                                         bool\n",
       "alternative_titles                          object\n",
       "backdrop_path                               object\n",
       "belongs_to_collection                      float32\n",
       "budget                                     float64\n",
       "genres                                      object\n",
       "homepage                                    object\n",
       "tmdbId                                       int64\n",
       "imdbId                                     float32\n",
       "keywords                                    object\n",
       "original_language                         category\n",
       "original_title                              object\n",
       "overview                                    object\n",
       "popularity                                 float64\n",
       "poster_path                                 object\n",
       "production_companies                        object\n",
       "production_countries                        object\n",
       "release_date                        datetime64[ns]\n",
       "revenue                                    float64\n",
       "runtime                                    float32\n",
       "spoken_languages                            object\n",
       "status                                    category\n",
       "tagline                                     object\n",
       "title                                       object\n",
       "video                                         bool\n",
       "vote_average                               float64\n",
       "vote_count                                   int64\n",
       "facebook_id                                 object\n",
       "instagram_id                                object\n",
       "twitter_id                                  object\n",
       "                                         ...      \n",
       "fractional_mean_order_cast_mean            float64\n",
       "fractional_mean_order_crew_lead            float64\n",
       "fractional_mean_order_cast_lead            float64\n",
       "fractional_mean_order_crew_lead5           float64\n",
       "fractional_mean_order_cast_lead5           float64\n",
       "importulence_crew_mean                     float64\n",
       "importulence_cast_mean                     float64\n",
       "importulence_crew_lead                     float64\n",
       "importulence_cast_lead                     float64\n",
       "importulence_crew_lead5                    float64\n",
       "importulence_cast_lead5                    float64\n",
       "genres__Science Fiction                    float32\n",
       "genres__Adventure                          float32\n",
       "genres__Family                             float32\n",
       "genres__Fantasy                            float32\n",
       "genres__Action                             float32\n",
       "genres__Thriller                           float32\n",
       "genres__History                            float32\n",
       "genres__Romance                            float32\n",
       "genres__Drama                              float32\n",
       "genres__Comedy                             float32\n",
       "genres__Horror                             float32\n",
       "genres__War                                float32\n",
       "genres__Mystery                            float32\n",
       "genres__Music                              float32\n",
       "genres__Crime                              float32\n",
       "genres__Western                            float32\n",
       "genres__Animation                          float32\n",
       "genres__Documentary                        float32\n",
       "genres__TV Movie                           float32\n",
       "Length: 95, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467917, 95)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not going to save this again as it's already huge/most of the size of the integrated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender_guess_integrated_crew_mean',\n",
       "       'gender_guess_integrated_cast_mean',\n",
       "       'gender_guess_integrated_crew_lead',\n",
       "       'gender_guess_integrated_cast_lead',\n",
       "       'gender_guess_integrated_crew_lead5',\n",
       "       'gender_guess_integrated_cast_lead5', 'credit_number_crew_mean',\n",
       "       'credit_number_cast_mean', 'credit_number_crew_lead',\n",
       "       'credit_number_cast_lead', 'credit_number_crew_lead5',\n",
       "       'credit_number_cast_lead5', 'job_number_crew_mean',\n",
       "       'job_number_crew_lead', 'job_number_crew_lead5',\n",
       "       'department_number_crew_mean', 'department_number_crew_lead',\n",
       "       'department_number_crew_lead5', 'mean_order_crew_mean',\n",
       "       'mean_order_cast_mean', 'mean_order_crew_lead', 'mean_order_cast_lead',\n",
       "       'mean_order_crew_lead5', 'mean_order_cast_lead5',\n",
       "       'fractional_mean_order_crew_mean', 'fractional_mean_order_cast_mean',\n",
       "       'fractional_mean_order_crew_lead', 'fractional_mean_order_cast_lead',\n",
       "       'fractional_mean_order_crew_lead5', 'fractional_mean_order_cast_lead5',\n",
       "       'importulence_crew_mean', 'importulence_cast_mean',\n",
       "       'importulence_crew_lead', 'importulence_cast_lead',\n",
       "       'importulence_crew_lead5', 'importulence_cast_lead5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TMB_cast_crew_aggregates.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender_guess_integrated_crew_mean     353522\n",
       "gender_guess_integrated_cast_mean     287414\n",
       "gender_guess_integrated_crew_lead     338539\n",
       "gender_guess_integrated_cast_lead     120486\n",
       "gender_guess_integrated_crew_lead5    353154\n",
       "gender_guess_integrated_cast_lead5    286647\n",
       "credit_number_crew_mean               371818\n",
       "credit_number_cast_mean               290774\n",
       "credit_number_crew_lead               371818\n",
       "credit_number_cast_lead               125246\n",
       "credit_number_crew_lead5              371818\n",
       "credit_number_cast_lead5              290408\n",
       "job_number_crew_mean                  340557\n",
       "job_number_crew_lead                  327208\n",
       "job_number_crew_lead5                 340259\n",
       "department_number_crew_mean           340557\n",
       "department_number_crew_lead           327208\n",
       "department_number_crew_lead5          340259\n",
       "mean_order_crew_mean                  340557\n",
       "mean_order_cast_mean                  273974\n",
       "mean_order_crew_lead                  327208\n",
       "mean_order_cast_lead                  115408\n",
       "mean_order_crew_lead5                 340259\n",
       "mean_order_cast_lead5                 271517\n",
       "fractional_mean_order_crew_mean       340557\n",
       "fractional_mean_order_cast_mean       273974\n",
       "fractional_mean_order_crew_lead       327208\n",
       "fractional_mean_order_cast_lead       115408\n",
       "fractional_mean_order_crew_lead5      340259\n",
       "fractional_mean_order_cast_lead5      271517\n",
       "importulence_crew_mean                318928\n",
       "importulence_cast_mean                273824\n",
       "importulence_crew_lead                297342\n",
       "importulence_cast_lead                114684\n",
       "importulence_crew_lead5               318589\n",
       "importulence_cast_lead5               271360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[TMB_cast_crew_aggregates.columns].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (8) themoviedatabase.com Kaggle DATASET: TMDB_KAGGLE\n",
    "dataset = \"The_Movies_Dataset\"\n",
    "\n",
    "# userId,movieId,rating,timestamp (26024289, 4)\n",
    "TMDB_KAGGLE_ratings = pd.read_csv(Dataset_directory+ dataset +os.sep+\"ratings.csv\")\n",
    "\n",
    "# movieId, imdbId, tmdbId (45843, 3)\n",
    "TMDB_KAGGLE_links_large = pd.read_csv(Dataset_directory+ dataset +os.sep+\"links.csv\")\n",
    "\n",
    "# movieId, imdbId, tmdbId (9125, 3)\n",
    "TMDB_KAGGLE_links_small = pd.read_csv(Dataset_directory+ dataset +os.sep+\"links_small.csv\")\n",
    "\n",
    "# movieId, imdbId, tmdbId (45853, 3) + 10 (prob causing overlaps!)\n",
    "TMDB_KAGGLE_links = TMDB_KAGGLE_links_large.append(TMDB_KAGGLE_links_small).drop_duplicates()\n",
    "\n",
    "TMDB_KAGGLE_links.drop([5173,7527],inplace=True)\n",
    "TMDB_KAGGLE_links[\"imdbId\"] = TMDB_KAGGLE_links[\"imdbId\"].astype('int')\n",
    "\n",
    "# TMB_links_amalgamated.isnull().sum()\n",
    "\n",
    "drop_indices = [13446, 16880, 5574, 9378, 29684, 20100, 14023, 16278, 848, 20065, 12131, 45641, 13676, 34144, 2594, 11217, 45197, 33499, 21093, 14089, 9215, 15871, 4390, 1485, 15165, 17350, 14077, 9631, 34061, 13289]\n",
    "TMDB_KAGGLE_links.loc[drop_indices,\"tmdbId\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "del TMDB_KAGGLE_ratings\n",
    "del TMB_cast_crew_aggregates,TMB_crew_individuals,TMB_cast_individuals,TMB_crew,TMB_cast,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## linking the whole dataset\n",
    "\n",
    "# SER_links -> movieId,imdbId,tmdbId (49174,  3)\n",
    "SER_links = SER_movies[[\"movieId\",\"imdbId\",\"tmdbId\"]].copy()\n",
    "# HETREC_links -> movieId,imdbId (10197,  2)\n",
    "HETREC_links = HETREC_movies[[\"movieId\",\"imdbId\"]].copy() # could add rtID on if you like\n",
    "# TMB_links -> tmdbId,imdbId (467917,  2)\n",
    "TMB_links = movies[[\"tmdbId\",\"imdbId\"]].copy()\n",
    "\n",
    "# ML_links -> movieId,imdbId,tmdbId (58098,  3)\n",
    "ML_links = ML_links[[\"movieId\",\"imdbId\",\"tmdbId\"]].copy()\n",
    "# TMDB_KAGGLE_links -> movieId,imdbId,tmdbId (45853,  3)\n",
    "\n",
    "# NUM -> pre-existing links to movieId\n",
    "# PERS -> movieId\n",
    "# LEARN -> movieId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = TMDB_KAGGLE_links.copy()\n",
    "all_links = all_links.append(ML_links,ignore_index=True,sort=False)\n",
    "all_links = all_links.append(TMB_links,ignore_index=True,sort=False)\n",
    "all_links = all_links.append(SER_links,ignore_index=True,sort=False)\n",
    "all_links = all_links.append(HETREC_links,ignore_index=True,sort=False)\n",
    "all_links = all_links.drop_duplicates()\n",
    "# movieId now unique (when you drop tmdbId nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_by_tmdbId = all_links[[\"movieId\",\"tmdbId\"]].dropna().drop_duplicates()\n",
    "movieId_by_tmdbId.index = movieId_by_tmdbId[\"movieId\"]\n",
    "null_tmbdIds = all_links[\"tmdbId\"].isnull()\n",
    "all_links.loc[null_tmbdIds, \"tmdbId\"] = all_links[ null_tmbdIds ].apply(lambda x: movieId_by_tmdbId.loc[x[\"movieId\"],\"tmdbId\"] if x[\"movieId\"] in movieId_by_tmdbId.index else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId    467917\n",
       "imdbId     132736\n",
       "tmdbId        977\n",
       "dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_links.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId    float64\n",
       "imdbId     float64\n",
       "tmdbId     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_links.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movies.drop(\"movieId\",axis=1, inplace=True)\n",
    "movies[\"tmdbId\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMDB_KAGGLE_links[\"movieId\"].loc[movies[\"tmdbId\"][0:100]]\n",
    "temp = TMDB_KAGGLE_links[[\"movieId\",\"tmdbId\"]].dropna().drop_duplicates(subset = [\"tmdbId\"])\n",
    "temp.index = temp[\"tmdbId\"]\n",
    "\n",
    "movies[\"movieId\"] = movies[\"tmdbId\"].apply(lambda x:temp[\"movieId\"].loc[x] if x in temp.index else np.nan )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a set of link unique in tmdbId\n",
    "tmdbId_links = all_links[all_links[\"tmdbId\"].notnull()].copy()\n",
    "tmdbId_links.index = range(0,tmdbId_links.shape[0])\n",
    "tmdbId_links.sort_values(by=\"movieId\", inplace=True)\n",
    "tmdbId_links.drop_duplicates(subset = [\"tmdbId\"],inplace=True)\n",
    "tmdbId_links.index = tmdbId_links[\"tmdbId\"]\n",
    "\n",
    "movies.loc[movies[\"movieId\"].isnull(),\"movieId\"] = movies.loc[movies[\"movieId\"].isnull(),\"tmdbId\"].apply(lambda x:tmdbId_links[\"movieId\"].loc[x] if x in tmdbId_links.index else np.nan )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del tmdbId_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410590"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[\"movieId\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies[\"movieId\"] = movies[\"movieId\"].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmdbId\n",
       "601        1097.0\n",
       "602         780.0\n",
       "603        2571.0\n",
       "604        6365.0\n",
       "605        6934.0\n",
       "606        1959.0\n",
       "607        1580.0\n",
       "608        5459.0\n",
       "609        1994.0\n",
       "612       41997.0\n",
       "613       31410.0\n",
       "614        5147.0\n",
       "615        7318.0\n",
       "616        7143.0\n",
       "617        1805.0\n",
       "618        7065.0\n",
       "619        3257.0\n",
       "620        2716.0\n",
       "621        1380.0\n",
       "622        3355.0\n",
       "623        1079.0\n",
       "624        3168.0\n",
       "625        1299.0\n",
       "626       25771.0\n",
       "627         778.0\n",
       "628         253.0\n",
       "629          50.0\n",
       "630         919.0\n",
       "631        8125.0\n",
       "632        3196.0\n",
       "           ...   \n",
       "522380        NaN\n",
       "522381        NaN\n",
       "522382        NaN\n",
       "522383        NaN\n",
       "522384        NaN\n",
       "523146        NaN\n",
       "523147        NaN\n",
       "523148        NaN\n",
       "578560        NaN\n",
       "578561        NaN\n",
       "607106        NaN\n",
       "578562        NaN\n",
       "607107        NaN\n",
       "607148        NaN\n",
       "578563        NaN\n",
       "607153        NaN\n",
       "607154        NaN\n",
       "607155        NaN\n",
       "607156        NaN\n",
       "578564        NaN\n",
       "578565        NaN\n",
       "578566        NaN\n",
       "606067        NaN\n",
       "606068        NaN\n",
       "606069        NaN\n",
       "578553        NaN\n",
       "578554        NaN\n",
       "578556        NaN\n",
       "578557        NaN\n",
       "578558        NaN\n",
       "Name: movieId, Length: 467917, dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[\"movieId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "INT_df = movies.copy()\n",
    "INT_df.columns = [\"TMB_\"+x for x in INT_df.columns]\n",
    "INT_df[\"movieId\"] = INT_df['TMB_movieId']\n",
    "\n",
    "temp = HETREC_movies.copy()\n",
    "temp.columns = [\"HETREC_\"+x for x in temp.columns]\n",
    "temp.index.name = \"movieId\"\n",
    "temp[\"HETREC_dataset\"] = True\n",
    "INT_df = INT_df.merge(temp, how ='outer', on ='movieId')\n",
    "\n",
    "temp = SER_movies.copy()\n",
    "temp.columns = [\"SER_\"+x for x in temp.columns]\n",
    "temp.index.name = \"movieId\"\n",
    "temp[\"SER_dataset\"] = True\n",
    "INT_df = INT_df.merge(temp, how ='outer', on ='movieId')\n",
    "\n",
    "temp = PERS_ratings_and_big5.copy()\n",
    "temp.columns = [\"PERS_\"+x for x in temp.columns]\n",
    "temp.index.name = \"movieId\"\n",
    "temp[\"PERS_dataset\"] = True\n",
    "INT_df = INT_df.merge(temp, how ='outer', on ='movieId')\n",
    "\n",
    "temp = ML_movies.copy()\n",
    "temp.columns = [\"ML_\"+x for x in temp.columns]\n",
    "temp.index.name = \"movieId\"\n",
    "temp[\"ML_dataset\"] = True\n",
    "INT_df = INT_df.merge(temp, how ='outer', on ='movieId')\n",
    "\n",
    "temp = NUM_movies[NUM_movies[\"movieId\"].notnull()].copy()\n",
    "temp.index = temp[\"movieId\"]\n",
    "temp.columns = [\"NUM_\"+x for x in temp.columns]\n",
    "temp[\"NUM_dataset\"] = True\n",
    "INT_df = INT_df.merge(temp, how ='outer', on ='movieId')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# INT_df.to_csv(Dataset_directory+\"Processed\"+os.sep+\"INT_df\"+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469996, 265)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INT_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdbId_vars = search(INT_df,\"tmdbId\").index\n",
    "INT_df[\"tmdbId\"]  = INT_df[ tmdbId_vars ].fillna(method=\"ffill\",axis=1)[tmdbId_vars[-1]]\n",
    "# movieId_vars = search(INT_df,\"movieId\").index\n",
    "# INT_df[\"movieId\"] = INT_df[ movieId_vars ].fillna(method=\"ffill\",axis=1)[movieId_vars[-1]]\n",
    "INT_df[\"uId\"] =  INT_df[\"tmdbId\"].apply(lambda x: str(int(x)) if pd.notnull(x) else \"NA\")+\"-\"\\\n",
    "                +INT_df[\"movieId\"].apply(lambda x: str(int(x)) if pd.notnull(x) else \"NA\")\n",
    "\n",
    "# amalgamated tmbdId and movieIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541400-NA        1\n",
       "549289-NA        1\n",
       "196288-NA        1\n",
       "300693-148482    1\n",
       "239832-NA        1\n",
       "573360-NA        1\n",
       "190564-NA        1\n",
       "371534-NA        1\n",
       "193596-NA        1\n",
       "291046-NA        1\n",
       "45978-25869      1\n",
       "593183-NA        1\n",
       "73924-177069     1\n",
       "272494-NA        1\n",
       "610838-NA        1\n",
       "66558-NA         1\n",
       "317304-NA        1\n",
       "557857-NA        1\n",
       "486257-NA        1\n",
       "549809-NA        1\n",
       "428949-NA        1\n",
       "297284-NA        1\n",
       "569816-NA        1\n",
       "318456-NA        1\n",
       "517538-NA        1\n",
       "438281-NA        1\n",
       "413775-NA        1\n",
       "322888-NA        1\n",
       "303192-NA        1\n",
       "265012-NA        1\n",
       "                ..\n",
       "464366-NA        1\n",
       "546922-NA        1\n",
       "379459-NA        1\n",
       "253865-137355    1\n",
       "127788-NA        1\n",
       "349877-NA        1\n",
       "456208-NA        1\n",
       "4925-4582        1\n",
       "298949-NA        1\n",
       "567201-NA        1\n",
       "445051-NA        1\n",
       "352357-NA        1\n",
       "531320-NA        1\n",
       "103119-124669    1\n",
       "607524-NA        1\n",
       "374482-NA        1\n",
       "464716-NA        1\n",
       "463682-NA        1\n",
       "41616-85770      1\n",
       "16032-109053     1\n",
       "379860-NA        1\n",
       "611634-NA        1\n",
       "192794-NA        1\n",
       "292328-NA        1\n",
       "460913-NA        1\n",
       "335305-NA        1\n",
       "356185-NA        1\n",
       "253743-NA        1\n",
       "523152-NA        1\n",
       "287453-NA        1\n",
       "Name: uId, Length: 469996, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actually unique!\n",
    "INT_df[\"uId\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict( json.loads(res) )\n",
    "# ~7 hrs -> 86,000 rows out of 450,000\n",
    "# 36.4 -> another 30hrs\n",
    "# 1/5 -> 300MB -> 1.5Gb (maybe save to other hd!)\n",
    "# ram not an issue - 200MB for tmbd_df\n",
    "\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display( HTML( df.to_html().replace(\"\\\\n\",\"<br>\") ) )\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' By Fred Cirera, after https://stackoverflow.com/a/1094933/1870254'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def memory_use(locs = locals().items()):\n",
    "    gc.collect()\n",
    "    # locals().items()\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name,value in locs),\n",
    "                             key= lambda x: -x[1])[:10]:\n",
    "        print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))\n",
    "        \n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# memory_use(locs = locals().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT_df.columns[INT_df.dtypes=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ind = INT_df[INT_df[\"uId\"]==\"527218-NA\"].index\n",
    "INT_df.drop(drop_ind,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_tmdb_dir = \"E:\\\\Datasets\\\\themoviedb\"\n",
    "INT_df.to_msgpack(secondary_tmdb_dir+os.sep+\"INT_df.msgpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_tmdb_dir = \"E:\\\\Datasets\\\\themoviedb\"\n",
    "INT_df = pd.read_msgpack(secondary_tmdb_dir+os.sep+\"INT_df.msgpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  (zlib or blosc)\n",
    "\n",
    "# INT_df.to_msgpack(secondary_tmdb_dir+os.sep+\"INT_df.msgpack.zlib\",compress = 'zlib')\n",
    "# # INT_df.to_msgpack(secondary_tmdb_dir+os.sep+\"INT_df.msgpack.blosc\",compress = 'blosc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow\n",
    "\n",
    "# INT_df.to_parquet(secondary_tmdb_dir+os.sep+\"INT_df.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT_df[\"TMB_tmdbId\"].value_counts()\n",
    "# TMB_video\n",
    "# HETREC_country -> category?\n",
    "# TMB_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pyarrow -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## losslessly compress dataframe\n",
    "\n",
    "# include option not to use null datatypes (older panda setups won't be able to read them!)\n",
    "\n",
    "def compress_df(df):\n",
    "    for col in df:\n",
    "        print(col, mem_usage(df[col]) )\n",
    "        dt = df[col].dtype\n",
    "        has_nans = df[col].isnull().sum()>0\n",
    "\n",
    "        if dt==\"object\":\n",
    "            descr = df[col].describe()\n",
    "            # case where boolean with nulls gets turn into an object\n",
    "            if (descr[\"unique\"]==2) and (True in df[col].unique()) and (False in df[col].unique()):\n",
    "                if has_nans:\n",
    "                    df[col] = df[col].astype('float32')\n",
    "                else:\n",
    "                    df[col] = df[col].astype('uint8')\n",
    "            elif descr[\"unique\"]<(descr[\"count\"]/2):\n",
    "                df[col] = df[col].astype('category')\n",
    "        elif dt in [\"int16\",\"int32\",\"int64\"]:\n",
    "            df[col] = df[col].apply(pd.to_numeric,downcast='signed')\n",
    "        elif dt in [\"uint16\",\"uint32\",\"uint64\"]:\n",
    "            df[col] = df[col].apply(pd.to_numeric,downcast='unsigned')\n",
    "        elif dt == \"float64\":\n",
    "            df[col] = df[col].apply(pd.to_numeric,downcast='float')\n",
    "        else:\n",
    "            continue\n",
    "        print(col, \"afer:\", mem_usage(df[col]) )\n",
    "            \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMB_adult 18.02 MB\n",
      "TMB_adult afer: 5.38 MB\n",
      "TMB_alternative_titles 19.85 MB\n",
      "TMB_alternative_titles afer: 15.68 MB\n",
      "TMB_backdrop_path 24.14 MB\n",
      "TMB_backdrop_path afer: 24.14 MB\n",
      "TMB_belongs_to_collection 5.38 MB\n",
      "TMB_budget 7.17 MB\n",
      "TMB_budget afer: 7.17 MB\n",
      "TMB_genres 21.51 MB\n",
      "TMB_genres afer: 5.31 MB\n",
      "TMB_homepage 17.91 MB\n",
      "TMB_homepage afer: 17.91 MB\n",
      "TMB_tmdbId 7.17 MB\n",
      "TMB_tmdbId afer: 7.17 MB\n",
      "TMB_imdbId 5.38 MB\n",
      "TMB_keywords 21.68 MB\n",
      "TMB_keywords afer: 13.90 MB\n",
      "TMB_original_language 4.50 MB\n",
      "TMB_original_title 40.46 MB\n",
      "TMB_original_title afer: 40.46 MB\n",
      "TMB_overview 153.22 MB\n",
      "TMB_overview afer: 153.22 MB\n",
      "TMB_popularity 7.17 MB\n",
      "TMB_popularity afer: 7.17 MB\n",
      "TMB_poster_path 37.53 MB\n",
      "TMB_poster_path afer: 37.53 MB\n",
      "TMB_production_companies 20.23 MB\n",
      "TMB_production_companies afer: 12.16 MB\n",
      "TMB_production_countries 20.24 MB\n",
      "TMB_production_countries afer: 5.15 MB\n",
      "TMB_release_date 7.17 MB\n",
      "TMB_revenue 7.17 MB\n",
      "TMB_revenue afer: 7.17 MB\n",
      "TMB_runtime 5.38 MB\n",
      "TMB_spoken_languages 20.30 MB\n",
      "TMB_spoken_languages afer: 4.89 MB\n",
      "TMB_status 4.03 MB\n",
      "TMB_tagline 22.84 MB\n",
      "TMB_tagline afer: 22.84 MB\n",
      "TMB_title 39.02 MB\n",
      "TMB_title afer: 39.02 MB\n",
      "TMB_video 18.01 MB\n",
      "TMB_video afer: 5.38 MB\n",
      "TMB_vote_average 7.17 MB\n",
      "TMB_vote_average afer: 7.17 MB\n",
      "TMB_vote_count 7.17 MB\n",
      "TMB_vote_count afer: 7.17 MB\n",
      "TMB_facebook_id 18.15 MB\n",
      "TMB_facebook_id afer: 18.15 MB\n",
      "TMB_instagram_id 18.01 MB\n",
      "TMB_instagram_id afer: 18.01 MB\n",
      "TMB_twitter_id 18.03 MB\n",
      "TMB_twitter_id afer: 18.03 MB\n",
      "TMB_num_genres 7.17 MB\n",
      "TMB_num_genres afer: 7.17 MB\n",
      "TMB_num_production_companies 7.17 MB\n",
      "TMB_num_production_companies afer: 7.17 MB\n",
      "TMB_num_production_countries 7.17 MB\n",
      "TMB_num_production_countries afer: 7.17 MB\n",
      "TMB_num_spoken_languages 7.17 MB\n",
      "TMB_num_spoken_languages afer: 7.17 MB\n",
      "TMB_num_alternative_titles 7.17 MB\n",
      "TMB_num_alternative_titles afer: 7.17 MB\n",
      "TMB_num_keywords 7.17 MB\n",
      "TMB_num_keywords afer: 7.17 MB\n",
      "TMB_cast_size 7.17 MB\n",
      "TMB_cast_size afer: 7.17 MB\n",
      "TMB_crew_size 7.17 MB\n",
      "TMB_crew_size afer: 7.17 MB\n",
      "TMB_cast_credit_list 39.15 MB\n",
      "TMB_cast_credit_list afer: 39.15 MB\n",
      "TMB_crew_credit_list 32.57 MB\n",
      "TMB_crew_credit_list afer: 32.57 MB\n",
      "TMB_gender_guess_integrated_crew_mean 7.17 MB\n",
      "TMB_gender_guess_integrated_crew_mean afer: 7.17 MB\n",
      "TMB_gender_guess_integrated_cast_mean 7.17 MB\n",
      "TMB_gender_guess_integrated_cast_mean afer: 7.17 MB\n",
      "TMB_gender_guess_integrated_crew_lead 7.17 MB\n",
      "TMB_gender_guess_integrated_crew_lead afer: 7.17 MB\n",
      "TMB_gender_guess_integrated_cast_lead 7.17 MB\n",
      "TMB_gender_guess_integrated_cast_lead afer: 7.17 MB\n",
      "TMB_gender_guess_integrated_crew_lead5 7.17 MB\n",
      "TMB_gender_guess_integrated_crew_lead5 afer: 7.17 MB\n",
      "TMB_gender_guess_integrated_cast_lead5 7.17 MB\n",
      "TMB_gender_guess_integrated_cast_lead5 afer: 7.17 MB\n",
      "TMB_credit_number_crew_mean 7.17 MB\n",
      "TMB_credit_number_crew_mean afer: 7.17 MB\n",
      "TMB_credit_number_cast_mean 7.17 MB\n",
      "TMB_credit_number_cast_mean afer: 7.17 MB\n",
      "TMB_credit_number_crew_lead 7.17 MB\n",
      "TMB_credit_number_crew_lead afer: 7.17 MB\n",
      "TMB_credit_number_cast_lead 7.17 MB\n",
      "TMB_credit_number_cast_lead afer: 7.17 MB\n",
      "TMB_credit_number_crew_lead5 7.17 MB\n",
      "TMB_credit_number_crew_lead5 afer: 7.17 MB\n",
      "TMB_credit_number_cast_lead5 7.17 MB\n",
      "TMB_credit_number_cast_lead5 afer: 7.17 MB\n",
      "TMB_job_number_crew_mean 7.17 MB\n",
      "TMB_job_number_crew_mean afer: 7.17 MB\n",
      "TMB_job_number_crew_lead 7.17 MB\n",
      "TMB_job_number_crew_lead afer: 7.17 MB\n",
      "TMB_job_number_crew_lead5 7.17 MB\n",
      "TMB_job_number_crew_lead5 afer: 7.17 MB\n",
      "TMB_department_number_crew_mean 7.17 MB\n",
      "TMB_department_number_crew_mean afer: 7.17 MB\n",
      "TMB_department_number_crew_lead 7.17 MB\n",
      "TMB_department_number_crew_lead afer: 7.17 MB\n",
      "TMB_department_number_crew_lead5 7.17 MB\n",
      "TMB_department_number_crew_lead5 afer: 7.17 MB\n",
      "TMB_mean_order_crew_mean 7.17 MB\n",
      "TMB_mean_order_crew_mean afer: 7.17 MB\n",
      "TMB_mean_order_cast_mean 7.17 MB\n",
      "TMB_mean_order_cast_mean afer: 7.17 MB\n",
      "TMB_mean_order_crew_lead 7.17 MB\n",
      "TMB_mean_order_crew_lead afer: 7.17 MB\n",
      "TMB_mean_order_cast_lead 7.17 MB\n",
      "TMB_mean_order_cast_lead afer: 7.17 MB\n",
      "TMB_mean_order_crew_lead5 7.17 MB\n",
      "TMB_mean_order_crew_lead5 afer: 7.17 MB\n",
      "TMB_mean_order_cast_lead5 7.17 MB\n",
      "TMB_mean_order_cast_lead5 afer: 7.17 MB\n",
      "TMB_fractional_mean_order_crew_mean 7.17 MB\n",
      "TMB_fractional_mean_order_crew_mean afer: 7.17 MB\n",
      "TMB_fractional_mean_order_cast_mean 7.17 MB\n",
      "TMB_fractional_mean_order_cast_mean afer: 7.17 MB\n",
      "TMB_fractional_mean_order_crew_lead 7.17 MB\n",
      "TMB_fractional_mean_order_crew_lead afer: 7.17 MB\n",
      "TMB_fractional_mean_order_cast_lead 7.17 MB\n",
      "TMB_fractional_mean_order_cast_lead afer: 7.17 MB\n",
      "TMB_fractional_mean_order_crew_lead5 7.17 MB\n",
      "TMB_fractional_mean_order_crew_lead5 afer: 7.17 MB\n",
      "TMB_fractional_mean_order_cast_lead5 7.17 MB\n",
      "TMB_fractional_mean_order_cast_lead5 afer: 7.17 MB\n",
      "TMB_importulence_crew_mean 7.17 MB\n",
      "TMB_importulence_crew_mean afer: 7.17 MB\n",
      "TMB_importulence_cast_mean 7.17 MB\n",
      "TMB_importulence_cast_mean afer: 7.17 MB\n",
      "TMB_importulence_crew_lead 7.17 MB\n",
      "TMB_importulence_crew_lead afer: 7.17 MB\n",
      "TMB_importulence_cast_lead 7.17 MB\n",
      "TMB_importulence_cast_lead afer: 7.17 MB\n",
      "TMB_importulence_crew_lead5 7.17 MB\n",
      "TMB_importulence_crew_lead5 afer: 7.17 MB\n",
      "TMB_importulence_cast_lead5 7.17 MB\n",
      "TMB_importulence_cast_lead5 afer: 7.17 MB\n",
      "TMB_genres__Science Fiction 5.38 MB\n",
      "TMB_genres__Adventure 5.38 MB\n",
      "TMB_genres__Family 5.38 MB\n",
      "TMB_genres__Fantasy 5.38 MB\n",
      "TMB_genres__Action 5.38 MB\n",
      "TMB_genres__Thriller 5.38 MB\n",
      "TMB_genres__History 5.38 MB\n",
      "TMB_genres__Romance 5.38 MB\n",
      "TMB_genres__Drama 5.38 MB\n",
      "TMB_genres__Comedy 5.38 MB\n",
      "TMB_genres__Horror 5.38 MB\n",
      "TMB_genres__War 5.38 MB\n",
      "TMB_genres__Mystery 5.38 MB\n",
      "TMB_genres__Music 5.38 MB\n",
      "TMB_genres__Crime 5.38 MB\n",
      "TMB_genres__Western 5.38 MB\n",
      "TMB_genres__Animation 5.38 MB\n",
      "TMB_genres__Documentary 5.38 MB\n",
      "TMB_genres__TV Movie 5.38 MB\n",
      "TMB_movieId 7.17 MB\n",
      "TMB_movieId afer: 7.17 MB\n",
      "movieId 7.17 MB\n",
      "movieId afer: 7.17 MB\n",
      "HETREC_movieId 7.17 MB\n",
      "HETREC_movieId afer: 7.17 MB\n",
      "HETREC_title 18.34 MB\n",
      "HETREC_title afer: 18.34 MB\n",
      "HETREC_imdbId 7.17 MB\n",
      "HETREC_imdbId afer: 7.17 MB\n",
      "HETREC_spanishTitle 18.40 MB\n",
      "HETREC_spanishTitle afer: 18.40 MB\n",
      "HETREC_imdbPictureURL 19.23 MB\n",
      "HETREC_imdbPictureURL afer: 19.23 MB\n",
      "HETREC_year 7.17 MB\n",
      "HETREC_year afer: 7.17 MB\n",
      "HETREC_rtID 18.32 MB\n",
      "HETREC_rtID afer: 18.32 MB\n",
      "HETREC_rtAllCriticsRating 7.17 MB\n",
      "HETREC_rtAllCriticsRating afer: 7.17 MB\n",
      "HETREC_rtAllCriticsNumReviews 7.17 MB\n",
      "HETREC_rtAllCriticsNumReviews afer: 7.17 MB\n",
      "HETREC_rtAllCriticsNumFresh 7.17 MB\n",
      "HETREC_rtAllCriticsNumFresh afer: 7.17 MB\n",
      "HETREC_rtAllCriticsNumRotten 7.17 MB\n",
      "HETREC_rtAllCriticsNumRotten afer: 7.17 MB\n",
      "HETREC_rtAllCriticsScore 7.17 MB\n",
      "HETREC_rtAllCriticsScore afer: 7.17 MB\n",
      "HETREC_rtTopCriticsRating 7.17 MB\n",
      "HETREC_rtTopCriticsRating afer: 7.17 MB\n",
      "HETREC_rtTopCriticsNumReviews 7.17 MB\n",
      "HETREC_rtTopCriticsNumReviews afer: 7.17 MB\n",
      "HETREC_rtTopCriticsNumFresh 7.17 MB\n",
      "HETREC_rtTopCriticsNumFresh afer: 7.17 MB\n",
      "HETREC_rtTopCriticsNumRotten 7.17 MB\n",
      "HETREC_rtTopCriticsNumRotten afer: 7.17 MB\n",
      "HETREC_rtTopCriticsScore 7.17 MB\n",
      "HETREC_rtTopCriticsScore afer: 7.17 MB\n",
      "HETREC_rtAudienceRating 7.17 MB\n",
      "HETREC_rtAudienceRating afer: 7.17 MB\n",
      "HETREC_rtAudienceNumRatings 7.17 MB\n",
      "HETREC_rtAudienceNumRatings afer: 7.17 MB\n",
      "HETREC_rtAudienceScore 7.17 MB\n",
      "HETREC_rtAudienceScore afer: 7.17 MB\n",
      "HETREC_rtPictureURL 18.72 MB\n",
      "HETREC_rtPictureURL afer: 18.72 MB\n",
      "HETREC_country 18.21 MB\n",
      "HETREC_country afer: 4.04 MB\n",
      "HETREC_directorID 18.30 MB\n",
      "HETREC_directorID afer: 4.91 MB\n",
      "HETREC_directorName 18.31 MB\n",
      "HETREC_directorName afer: 4.91 MB\n",
      "HETREC_ratings_n 7.17 MB\n",
      "HETREC_ratings_n afer: 7.17 MB\n",
      "HETREC_ratings_mean 7.17 MB\n",
      "HETREC_ratings_mean afer: 7.17 MB\n",
      "HETREC_ratings_std 7.17 MB\n",
      "HETREC_ratings_std afer: 7.17 MB\n",
      "HETREC_rtAllCriticsFracRotten 7.17 MB\n",
      "HETREC_rtAllCriticsFracRotten afer: 7.17 MB\n",
      "HETREC_rtTopCriticsFracRotten 7.17 MB\n",
      "HETREC_rtTopCriticsFracRotten afer: 7.17 MB\n",
      "HETREC_genres__Adventure 7.17 MB\n",
      "HETREC_genres__Adventure afer: 7.17 MB\n",
      "HETREC_genres__Animation 7.17 MB\n",
      "HETREC_genres__Animation afer: 7.17 MB\n",
      "HETREC_genres__Children 7.17 MB\n",
      "HETREC_genres__Children afer: 7.17 MB\n",
      "HETREC_genres__Comedy 7.17 MB\n",
      "HETREC_genres__Comedy afer: 7.17 MB\n",
      "HETREC_genres__Fantasy 7.17 MB\n",
      "HETREC_genres__Fantasy afer: 7.17 MB\n",
      "HETREC_genres__Romance 7.17 MB\n",
      "HETREC_genres__Romance afer: 7.17 MB\n",
      "HETREC_genres__Drama 7.17 MB\n",
      "HETREC_genres__Drama afer: 7.17 MB\n",
      "HETREC_genres__Action 7.17 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HETREC_genres__Action afer: 7.17 MB\n",
      "HETREC_genres__Crime 7.17 MB\n",
      "HETREC_genres__Crime afer: 7.17 MB\n",
      "HETREC_genres__Thriller 7.17 MB\n",
      "HETREC_genres__Thriller afer: 7.17 MB\n",
      "HETREC_genres__Horror 7.17 MB\n",
      "HETREC_genres__Horror afer: 7.17 MB\n",
      "HETREC_genres__Mystery 7.17 MB\n",
      "HETREC_genres__Mystery afer: 7.17 MB\n",
      "HETREC_genres__Sci-Fi 7.17 MB\n",
      "HETREC_genres__Sci-Fi afer: 7.17 MB\n",
      "HETREC_genres__IMAX 7.17 MB\n",
      "HETREC_genres__IMAX afer: 7.17 MB\n",
      "HETREC_genres__Documentary 7.17 MB\n",
      "HETREC_genres__Documentary afer: 7.17 MB\n",
      "HETREC_genres__War 7.17 MB\n",
      "HETREC_genres__War afer: 7.17 MB\n",
      "HETREC_genres__Musical 7.17 MB\n",
      "HETREC_genres__Musical afer: 7.17 MB\n",
      "HETREC_genres__Film-Noir 7.17 MB\n",
      "HETREC_genres__Film-Noir afer: 7.17 MB\n",
      "HETREC_genres__Western 7.17 MB\n",
      "HETREC_genres__Western afer: 7.17 MB\n",
      "HETREC_genres__Short 7.17 MB\n",
      "HETREC_genres__Short afer: 7.17 MB\n",
      "HETREC_dataset 17.97 MB\n",
      "HETREC_dataset afer: 4.03 MB\n",
      "SER_movieId 7.17 MB\n",
      "SER_movieId afer: 7.17 MB\n",
      "SER_title 20.38 MB\n",
      "SER_title afer: 20.38 MB\n",
      "SER_releaseDate 19.57 MB\n",
      "SER_releaseDate afer: 6.15 MB\n",
      "SER_imdbId 7.17 MB\n",
      "SER_imdbId afer: 7.17 MB\n",
      "SER_tmdbId 7.17 MB\n",
      "SER_tmdbId afer: 7.17 MB\n",
      "SER_director_list 18.31 MB\n",
      "SER_director_list afer: 5.90 MB\n",
      "SER_num_directors 7.17 MB\n",
      "SER_num_directors afer: 7.17 MB\n",
      "SER_num_genres 7.17 MB\n",
      "SER_num_genres afer: 7.17 MB\n",
      "SER_starring_list 19.64 MB\n",
      "SER_starring_list afer: 19.64 MB\n",
      "SER_num_starring 7.17 MB\n",
      "SER_num_starring afer: 7.17 MB\n",
      "SER_genres__Adventure 5.38 MB\n",
      "SER_genres__Animation 5.38 MB\n",
      "SER_genres__Children 5.38 MB\n",
      "SER_genres__Comedy 5.38 MB\n",
      "SER_genres__Fantasy 5.38 MB\n",
      "SER_genres__Romance 5.38 MB\n",
      "SER_genres__Drama 5.38 MB\n",
      "SER_genres__Action 5.38 MB\n",
      "SER_genres__Crime 5.38 MB\n",
      "SER_genres__Thriller 5.38 MB\n",
      "SER_genres__Horror 5.38 MB\n",
      "SER_genres__Mystery 5.38 MB\n",
      "SER_genres__Sci-Fi 5.38 MB\n",
      "SER_genres__IMAX 5.38 MB\n",
      "SER_genres__Documentary 5.38 MB\n",
      "SER_genres__War 5.38 MB\n",
      "SER_genres__Musical 5.38 MB\n",
      "SER_genres__Western 5.38 MB\n",
      "SER_genres__Film-Noir 5.38 MB\n",
      "SER_ratings_n 7.17 MB\n",
      "SER_ratings_n afer: 7.17 MB\n",
      "SER_ratings_mean 7.17 MB\n",
      "SER_ratings_mean afer: 7.17 MB\n",
      "SER_ratings_std 7.17 MB\n",
      "SER_ratings_std afer: 7.17 MB\n",
      "SER_rating 7.17 MB\n",
      "SER_rating afer: 7.17 MB\n",
      "SER_predictedRating 7.17 MB\n",
      "SER_predictedRating afer: 7.17 MB\n",
      "SER_s1 7.17 MB\n",
      "SER_s1 afer: 7.17 MB\n",
      "SER_s2 7.17 MB\n",
      "SER_s2 afer: 7.17 MB\n",
      "SER_s3 7.17 MB\n",
      "SER_s3 afer: 7.17 MB\n",
      "SER_s4 7.17 MB\n",
      "SER_s4 afer: 7.17 MB\n",
      "SER_s5 7.17 MB\n",
      "SER_s5 afer: 7.17 MB\n",
      "SER_s6 7.17 MB\n",
      "SER_s6 afer: 7.17 MB\n",
      "SER_s7 7.17 MB\n",
      "SER_s7 afer: 7.17 MB\n",
      "SER_s8 7.17 MB\n",
      "SER_s8 afer: 7.17 MB\n",
      "SER_q 7.17 MB\n",
      "SER_q afer: 7.17 MB\n",
      "SER_s_ser_rel 7.17 MB\n",
      "SER_s_ser_rel afer: 7.17 MB\n",
      "SER_s_ser_find 7.17 MB\n",
      "SER_s_ser_find afer: 7.17 MB\n",
      "SER_s_ser_imp 7.17 MB\n",
      "SER_s_ser_imp afer: 7.17 MB\n",
      "SER_s_ser_rec 7.17 MB\n",
      "SER_s_ser_rec afer: 7.17 MB\n",
      "SER_m_ser_rel 7.17 MB\n",
      "SER_m_ser_rel afer: 7.17 MB\n",
      "SER_m_ser_find 7.17 MB\n",
      "SER_m_ser_find afer: 7.17 MB\n",
      "SER_m_ser_imp 7.17 MB\n",
      "SER_m_ser_imp afer: 7.17 MB\n",
      "SER_m_ser_rec 7.17 MB\n",
      "SER_m_ser_rec afer: 7.17 MB\n",
      "SER_n 7.17 MB\n",
      "SER_n afer: 7.17 MB\n",
      "SER_dataset 18.12 MB\n",
      "SER_dataset afer: 4.03 MB\n",
      "PERS_openness_r 7.17 MB\n",
      "PERS_openness_r afer: 7.17 MB\n",
      "PERS_openness_p 7.17 MB\n",
      "PERS_openness_p afer: 7.17 MB\n",
      "PERS_openness_mean 7.17 MB\n",
      "PERS_openness_mean afer: 7.17 MB\n",
      "PERS_openness_std 7.17 MB\n",
      "PERS_openness_std afer: 7.17 MB\n",
      "PERS_agreeableness_r 7.17 MB\n",
      "PERS_agreeableness_r afer: 7.17 MB\n",
      "PERS_agreeableness_p 7.17 MB\n",
      "PERS_agreeableness_p afer: 7.17 MB\n",
      "PERS_agreeableness_mean 7.17 MB\n",
      "PERS_agreeableness_mean afer: 7.17 MB\n",
      "PERS_agreeableness_std 7.17 MB\n",
      "PERS_agreeableness_std afer: 7.17 MB\n",
      "PERS_emotional_stability_r 7.17 MB\n",
      "PERS_emotional_stability_r afer: 7.17 MB\n",
      "PERS_emotional_stability_p 7.17 MB\n",
      "PERS_emotional_stability_p afer: 7.17 MB\n",
      "PERS_emotional_stability_mean 7.17 MB\n",
      "PERS_emotional_stability_mean afer: 7.17 MB\n",
      "PERS_emotional_stability_std 7.17 MB\n",
      "PERS_emotional_stability_std afer: 7.17 MB\n",
      "PERS_conscientiousness_r 7.17 MB\n",
      "PERS_conscientiousness_r afer: 7.17 MB\n",
      "PERS_conscientiousness_p 7.17 MB\n",
      "PERS_conscientiousness_p afer: 7.17 MB\n",
      "PERS_conscientiousness_mean 7.17 MB\n",
      "PERS_conscientiousness_mean afer: 7.17 MB\n",
      "PERS_conscientiousness_std 7.17 MB\n",
      "PERS_conscientiousness_std afer: 7.17 MB\n",
      "PERS_extraversion_r 7.17 MB\n",
      "PERS_extraversion_r afer: 7.17 MB\n",
      "PERS_extraversion_p 7.17 MB\n",
      "PERS_extraversion_p afer: 7.17 MB\n",
      "PERS_extraversion_mean 7.17 MB\n",
      "PERS_extraversion_mean afer: 7.17 MB\n",
      "PERS_extraversion_std 7.17 MB\n",
      "PERS_extraversion_std afer: 7.17 MB\n",
      "PERS_ratings_n 7.17 MB\n",
      "PERS_ratings_n afer: 7.17 MB\n",
      "PERS_ratings_mean 7.17 MB\n",
      "PERS_ratings_mean afer: 7.17 MB\n",
      "PERS_ratings_std 7.17 MB\n",
      "PERS_ratings_std afer: 7.17 MB\n",
      "PERS_movieId 7.17 MB\n",
      "PERS_movieId afer: 7.17 MB\n",
      "PERS_dataset 18.06 MB\n",
      "PERS_dataset afer: 4.03 MB\n",
      "ML_movieId 7.17 MB\n",
      "ML_movieId afer: 7.17 MB\n",
      "ML_title 20.81 MB\n",
      "ML_title afer: 20.81 MB\n",
      "ML_genres__Adventure 7.17 MB\n",
      "ML_genres__Adventure afer: 7.17 MB\n",
      "ML_genres__Animation 7.17 MB\n",
      "ML_genres__Animation afer: 7.17 MB\n",
      "ML_genres__Children 7.17 MB\n",
      "ML_genres__Children afer: 7.17 MB\n",
      "ML_genres__Comedy 7.17 MB\n",
      "ML_genres__Comedy afer: 7.17 MB\n",
      "ML_genres__Fantasy 7.17 MB\n",
      "ML_genres__Fantasy afer: 7.17 MB\n",
      "ML_genres__Romance 7.17 MB\n",
      "ML_genres__Romance afer: 7.17 MB\n",
      "ML_genres__Drama 7.17 MB\n",
      "ML_genres__Drama afer: 7.17 MB\n",
      "ML_genres__Action 7.17 MB\n",
      "ML_genres__Action afer: 7.17 MB\n",
      "ML_genres__Crime 7.17 MB\n",
      "ML_genres__Crime afer: 7.17 MB\n",
      "ML_genres__Thriller 7.17 MB\n",
      "ML_genres__Thriller afer: 7.17 MB\n",
      "ML_genres__Horror 7.17 MB\n",
      "ML_genres__Horror afer: 7.17 MB\n",
      "ML_genres__Mystery 7.17 MB\n",
      "ML_genres__Mystery afer: 7.17 MB\n",
      "ML_genres__Sci-Fi 7.17 MB\n",
      "ML_genres__Sci-Fi afer: 7.17 MB\n",
      "ML_genres__IMAX 7.17 MB\n",
      "ML_genres__IMAX afer: 7.17 MB\n",
      "ML_genres__Documentary 7.17 MB\n",
      "ML_genres__Documentary afer: 7.17 MB\n",
      "ML_genres__War 7.17 MB\n",
      "ML_genres__War afer: 7.17 MB\n",
      "ML_genres__Musical 7.17 MB\n",
      "ML_genres__Musical afer: 7.17 MB\n",
      "ML_genres__Western 7.17 MB\n",
      "ML_genres__Western afer: 7.17 MB\n",
      "ML_genres__Film-Noir 7.17 MB\n",
      "ML_genres__Film-Noir afer: 7.17 MB\n",
      "ML_ratings_n 7.17 MB\n",
      "ML_ratings_n afer: 7.17 MB\n",
      "ML_ratings_mean 7.17 MB\n",
      "ML_ratings_mean afer: 7.17 MB\n",
      "ML_ratings_std 7.17 MB\n",
      "ML_ratings_std afer: 7.17 MB\n",
      "ML_imdbId 7.17 MB\n",
      "ML_imdbId afer: 7.17 MB\n",
      "ML_tmdbId 7.17 MB\n",
      "ML_tmdbId afer: 7.17 MB\n",
      "ML_dataset 18.15 MB\n",
      "ML_dataset afer: 4.03 MB\n",
      "NUM_title 18.13 MB\n",
      "NUM_title afer: 18.13 MB\n",
      "NUM_link 18.23 MB\n",
      "NUM_link afer: 18.23 MB\n",
      "NUM_production_budget 7.17 MB\n",
      "NUM_production_budget afer: 7.17 MB\n",
      "NUM_domestic_gross 7.17 MB\n",
      "NUM_domestic_gross afer: 7.17 MB\n",
      "NUM_worldwide_gross 7.17 MB\n",
      "NUM_worldwide_gross afer: 7.17 MB\n",
      "NUM_release_year 7.17 MB\n",
      "NUM_release_year afer: 7.17 MB\n",
      "NUM_release_month 7.17 MB\n",
      "NUM_release_month afer: 7.17 MB\n",
      "NUM_release_day 7.17 MB\n",
      "NUM_release_day afer: 7.17 MB\n",
      "NUM_movieId 7.17 MB\n",
      "NUM_movieId afer: 7.17 MB\n",
      "NUM_worldwide_gross_divided_by_budget 7.17 MB\n",
      "NUM_worldwide_gross_divided_by_budget afer: 7.17 MB\n",
      "NUM_international_gross 7.17 MB\n",
      "NUM_international_gross afer: 7.17 MB\n",
      "NUM_domestic_gross_fraction 7.17 MB\n",
      "NUM_domestic_gross_fraction afer: 7.17 MB\n",
      "NUM_dataset 17.95 MB\n",
      "NUM_dataset afer: 4.03 MB\n",
      "tmdbId 7.17 MB\n",
      "tmdbId afer: 7.17 MB\n",
      "uId 33.29 MB\n",
      "uId afer: 33.29 MB\n"
     ]
    }
   ],
   "source": [
    "INT_df2 = compress_df(INT_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descr[\"unique\"]<(descr[\"count\"]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = pd.Series( pd.array( INT_df[\"TMB_adult\"].astype('float32') , dtype=\"UInt8\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('18.02 MB', '0.90 MB')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage( INT_df[\"TMB_adult\"]), mem_usage( ad), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = INT_df[\"TMB_adult\"].copy()\n",
    "ad.drop(2,inplace=True)\n",
    "ad = pd.Series( pd.array( ad.astype('float32') , dtype=\"UInt8\"), ad.index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = person_ids\n",
    "for col in df.columns:\n",
    "    print(col, df[col].dtype, mem_usage(df[col]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    445758\n",
       "1     24237\n",
       "Name: TMB_adult, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.replace(np.nan,0.0).astype('uint8').value_counts()#.apply(pd.to_numeric,downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469996,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INT_df[\"TMB_adult\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.0\n",
       "1         0.0\n",
       "2         0.0\n",
       "3         0.0\n",
       "4         0.0\n",
       "5         0.0\n",
       "6         0.0\n",
       "7         0.0\n",
       "8         0.0\n",
       "9         0.0\n",
       "10        0.0\n",
       "11        0.0\n",
       "12        0.0\n",
       "13        0.0\n",
       "14        0.0\n",
       "15        0.0\n",
       "16        0.0\n",
       "17        0.0\n",
       "18        0.0\n",
       "19        0.0\n",
       "20        0.0\n",
       "21        0.0\n",
       "22        0.0\n",
       "23        0.0\n",
       "24        0.0\n",
       "25        0.0\n",
       "26        0.0\n",
       "27        0.0\n",
       "28        0.0\n",
       "29        0.0\n",
       "         ... \n",
       "469966    NaN\n",
       "469967    NaN\n",
       "469968    NaN\n",
       "469969    NaN\n",
       "469970    NaN\n",
       "469971    NaN\n",
       "469972    NaN\n",
       "469973    NaN\n",
       "469974    NaN\n",
       "469975    NaN\n",
       "469976    NaN\n",
       "469977    NaN\n",
       "469978    NaN\n",
       "469979    NaN\n",
       "469980    NaN\n",
       "469981    NaN\n",
       "469982    NaN\n",
       "469983    NaN\n",
       "469984    NaN\n",
       "469985    NaN\n",
       "469986    NaN\n",
       "469987    NaN\n",
       "469988    NaN\n",
       "469989    NaN\n",
       "469990    NaN\n",
       "469991    NaN\n",
       "469992    NaN\n",
       "469993    NaN\n",
       "469994    NaN\n",
       "469995    NaN\n",
       "Name: TMB_adult, Length: 469996, dtype: float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INT_df[\"TMB_adult\"].apply(pd.to_numeric,downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Cook_Tortoise_Imdb_project]",
   "language": "python",
   "name": "conda-env-Cook_Tortoise_Imdb_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
