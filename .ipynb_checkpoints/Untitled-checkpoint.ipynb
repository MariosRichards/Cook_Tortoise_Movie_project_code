{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic python data handling analysis modules\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import pickle, os, gc, re\n",
    "# small utility functions\n",
    "from utility import *\n",
    "\n",
    "# interactive jupyter widgets!\n",
    "# https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dataset_directory = \"..\"+os.sep+\"Datasets\"+os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INT_df = pd.read_msgpack(Dataset_directory+\"Processed\"+os.sep+\"INT_df_with_aggregate\"+\".msgpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_series_mislabelled_as_a_movie = INT_df[INT_df[\"TMB_tv_not_film\"]==1].index\n",
    "INT_df.drop(tv_series_mislabelled_as_a_movie,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    897\n",
       "object      35\n",
       "int64        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INT_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object_columns = INT_df.columns[INT_df.dtypes==\"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TMB_homepage', 'TMB_original_title', 'TMB_overview', 'TMB_poster_path',\n",
       "       'TMB_production_companies', 'TMB_production_countries',\n",
       "       'TMB_release_date', 'TMB_spoken_languages', 'TMB_status', 'TMB_tagline',\n",
       "       'TMB_title', 'TMB_keyword_id_list', 'TMB_cast_list', 'TMB_crew_list',\n",
       "       'HETREC_title', 'HETREC_spanishTitle', 'HETREC_imdbPictureURL',\n",
       "       'HETREC_rtID', 'HETREC_rtPictureURL', 'HETREC_country',\n",
       "       'HETREC_directorID', 'HETREC_directorName', 'SER_title',\n",
       "       'SER_releaseDate', 'SER_directedBy', 'SER_starring', 'SER_genres',\n",
       "       'SER_director_list', 'SER_genres_list', 'SER_starring_list', 'ML_title',\n",
       "       'ML_genres', 'NUM_date', 'NUM_title', 'NUM_link'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_dummy_prefixes = [\"original_language\",\"spoken_languages\",\"production_countries\",\n",
    "                          \"production_companies\",\"keyword\",\"cast\",\"crew\"]\n",
    "list_of_dummy_prefixes =[]\n",
    "\n",
    "INT_df_ord_only = INT_df.copy() # work with a safe copy\n",
    "\n",
    "object_columns = INT_df_ord_only.columns[INT_df.dtypes==\"object\"]\n",
    "INT_df_ord_only.drop(object_columns,axis=1,inplace=True)\n",
    "\n",
    "dummy_reg_exp = \"TMB_(\"+\"|\".join(list_of_dummy_prefixes)+\")_\"\n",
    "dummy_columns = [x for x in INT_df_ord_only.columns if re.match(dummy_reg_exp,x)]\n",
    "if \"TMB_crew_size\" in dummy_columns: dummy_columns.remove(\"TMB_crew_size\")\n",
    "if \"TMB_cast_size\" in dummy_columns: dummy_columns.remove(\"TMB_cast_size\") \n",
    "\n",
    "INT_df_ord_only.drop(dummy_columns,axis=1,inplace=True)\n",
    "\n",
    "id_columns = [x for x in INT_df_ord_only.columns if re.search(\"Id|_id|ID\",x)]\n",
    "id_columns.extend([\"TMB_collectionId\",\"TMB_original_language\",\"NUM_scrape_count\",\"NUM_page_no\"])\n",
    "INT_df_ord_only = INT_df_ord_only.drop(id_columns,axis=1)\n",
    "\n",
    "sample_size_variables = [x for x in INT_df_ord_only.columns if re.search(\"_n$|_n_x$|_n_y$\",x.lower())]\n",
    "INT_df_ord_only = INT_df_ord_only.drop(sample_size_variables,axis=1)\n",
    "\n",
    "# normalised_variables = [x for x in INT_df_ord_only.columns if re.search(\"normalised\",x.lower())]\n",
    "# INT_df_ord_only = INT_df_ord_only.drop(normalised_variables,axis=1)\n",
    "\n",
    "unnormalised_variables = [x.split(\"_normalised\")[0] for x in INT_df.columns if re.search(\"normalised\",x.lower())]\n",
    "INT_df_ord_only = INT_df_ord_only.drop(unnormalised_variables,axis=1)\n",
    "\n",
    "\n",
    "alternate_year_variables = [\"TMB_years_since_first_movie\",\"TMB_release_decade\",\"TMB_release_year\",\"NUM_release_year\",\"HETREC_year\",\n",
    "                            \"TMB_release_month\",\"NUM_release_month\",\"TMB_release_day\",\"NUM_release_day\"]\n",
    "INT_df_ord_only = INT_df_ord_only.drop(alternate_year_variables,axis=1)\n",
    "\n",
    "excess_gross_variables = ['NUM_domestic_gross','NUM_international_gross','NUM_worldwide_gross_divided_by_budget',\n",
    "                          'TMB_budget_normalised', 'TMB_revenue_normalised', 'TMB_revenue_divided_by_budget_normalised',\n",
    "                          'NUM_worldwide_gross',\n",
    "                          'NUM_production_budget']\n",
    "INT_df_ord_only = INT_df_ord_only.drop(excess_gross_variables,axis=1)\n",
    "\n",
    "\n",
    "excess_hetrec_variables = ['HETREC_rtAllCriticsNumFresh','HETREC_rtAllCriticsNumRotten',\n",
    "                           'HETREC_rtTopCriticsNumFresh','HETREC_rtTopCriticsNumRotten']\n",
    "INT_df_ord_only = INT_df_ord_only.drop(excess_hetrec_variables,axis=1)\n",
    "\n",
    "# p_values\n",
    "pers_p_values = [x for x in INT_df_ord_only.columns if re.search(\"_p$\",x.lower())]\n",
    "INT_df_ord_only = INT_df_ord_only.drop(pers_p_values,axis=1)\n",
    "\n",
    "# SER variables\n",
    "excess_ser_variables = [x for x in INT_df_ord_only.columns if re.search(\"SER_(predictedRating|s\\d|s_|q|m)\",x)]\n",
    "INT_df_ord_only = INT_df_ord_only.drop(excess_ser_variables,axis=1)\n",
    "\n",
    "# choose one of score/rating/fraction -> noise_variance suggests keep score, ditch rest\n",
    "excess_hetrec_variables = [x for x in INT_df_ord_only.columns if re.search(\"HETREC_rt\\w*(Rating$)|FractionFresh\",x)]\n",
    "INT_df_ord_only = INT_df_ord_only.drop(excess_hetrec_variables,axis=1)\n",
    "\n",
    "# \n",
    "too_low_variation = [\"TMB_adult\",\"TMB_video\"]\n",
    "INT_df_ord_only = INT_df_ord_only.drop(too_low_variation,axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59403, 818)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INT_df_ord_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corrank(X):\n",
    "    import itertools\n",
    "    df = pd.DataFrame([[(i,j),X.loc[i,j]] for i,j in list(itertools.combinations(X.corr(), 2))],columns=['pairs','corr'])    \n",
    "    print(df.sort_values(by='corr',ascending=False).dropna())\n",
    "\n",
    "naive_cross_corrs = INT_df_ord_only.corr()    \n",
    "    \n",
    "corrank(naive_cross_corrs) # prints a descending list of correlation pair (Max on top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needed for Factor Analysis\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, NMF, TruncatedSVD, FastICA, FactorAnalysis, SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = INT_df_ord_only.fillna(INT_df_ord_only.mean()).copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_subfolder = \"..\"+os.sep+\"Output\"+os.sep + \"numerical_data_only\" + os.sep\n",
    "if not os.path.exists( output_subfolder ):\n",
    "    os.makedirs( output_subfolder )\n",
    "    \n",
    "\n",
    "# standardise data (subtract out the mean, divide through by standard deviation)\n",
    "clean_feature_set_std = StandardScaler().fit_transform(df.values )\n",
    "BES_std = pd.DataFrame(      clean_feature_set_std,\n",
    "                             columns = df.columns,\n",
    "                             index   = df.index      )\n",
    "\n",
    "\n",
    "\n",
    "n_components = BES_std.shape[1]\n",
    "# n_components = 3\n",
    "\n",
    "decomp = FactorAnalysis(svd_method = 'lapack',n_components = n_components) ## ~10s ,n_components=30 -> 1.5 hrs\n",
    "decomp_method = str(decomp).split(\"(\")[0] \n",
    "# ,n_components=30\n",
    "\n",
    "X_r = decomp.fit_transform(BES_std)\n",
    "\n",
    "BES_decomp = pd.DataFrame(   X_r,\n",
    "                             columns = range(0,n_components),\n",
    "                             index   = df.index)\n",
    "\n",
    "\n",
    "\n",
    "load_suff = \"FactorAnalysis\"\n",
    "save = True # False => Load\n",
    "\n",
    "if save & ( 'decomp' in globals() ): # SAVE    ##( 'decomp' not in globals() )\n",
    "    decomp_method = str(decomp).split(\"(\")[0] \n",
    "    subdir = output_subfolder + decomp_method\n",
    "    fname = subdir+ os.sep + decomp_method\n",
    "    # create dir, save decomp object, BES_decomp, BES_std    \n",
    "#     if not os.path.exists(subdir): os.makedirs(subdir)\n",
    "#     with open(fname+\".pkl\", \"wb\") as f: pickle.dump( decomp, f )\n",
    "#     BES_decomp.to_hdf(fname+\".hdf\"        , decomp_method)\n",
    "#     BES_std.to_hdf(   fname+\"_std\"+\".hdf\" , decomp_method)\n",
    "    \n",
    "# else: # LOAD decomp results (default is SAVE)\n",
    "#     decomp_method = load_suff\n",
    "#     subdir = output_subfolder + os.sep + decomp_method    \n",
    "#     fname = subdir + os.sep + decomp_method\n",
    "#     if not os.path.exists(subdir): raise Exception(subdir + ' does not exist!')\n",
    "#     # load decomp object, BES_decomp, BES_std, n_components\n",
    "#     with open(fname+\".pkl\", \"rb\") as f: decomp = pickle.load(f) \n",
    "#     BES_decomp = pd.read_hdf(fname+\".hdf\")\n",
    "#     BES_std    = pd.read_hdf(fname+\"_std\"+\".hdf\")\n",
    "#     n_components = decomp.components_.shape[0] \n",
    "\n",
    "# n_components=44\n",
    "# BES_decomp = BES_decomp[BES_decomp.columns[0:44]]\n",
    "\n",
    "display_pca_data(n_components, decomp, BES_std)\n",
    "\n",
    "\n",
    "\n",
    "(BES_decomp, comp_labels, comp_dict) = display_components(n_components, decomp,\n",
    "                                                          df.columns, BES_decomp, manifest=None, \n",
    "                                                          save_folder = subdir,  \n",
    "                                                          show_first_x_comps= 10, show_histogram = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Treatment = \"xgboost_numerical_only_with_agg_crew_cast_data\"\n",
    "\n",
    "BES_output_folder= \"..\"+os.sep+\"Output\"+os.sep\n",
    "\n",
    "drop_cols = [\"gross_normalised\",\"budget_normalised\"]\n",
    "\n",
    "df = INT_df_ord_only.drop(drop_cols,axis=1).copy()\n",
    "drop_movies = df[df.isnull().all(axis=1)].index\n",
    "df.drop(drop_movies,inplace=True)\n",
    "\n",
    "# standardise data (subtract out the mean, divide through by standard deviation)\n",
    "df = pd.DataFrame(           StandardScaler().fit_transform(df.values ),\n",
    "                             columns = df.columns,\n",
    "                             index   = df.index      )\n",
    "\n",
    "var_name = \"gross_by_budget_normalised\"\n",
    "\n",
    "not_a_tv_series_mislabelled_as_a_movie = ~(INT_df_ord_only[\"TMB_tv_not_film\"]==1)\n",
    "\n",
    "mask = df[var_name].notnull() & not_a_tv_series_mislabelled_as_a_movie\n",
    "# df.drop([\"TMB_revenue_normalised\",\"TMB_budget_normalised\"],inplace=True)\n",
    "df = df[mask].copy()\n",
    "\n",
    "colname = var_name\n",
    "\n",
    "var_stub = var_name\n",
    "\n",
    "var_list = [var_name]\n",
    "var_stub_list = [var_stub]\n",
    "\n",
    "# BES_cens = BES_cens_base[fertility_columns+age_columns+[\"leaveHanretty\"]+[\"c11Degree\"]]\n",
    "(explainer, shap_values) = xgboost_run(subdir=colname,min_features = min(df.shape[1]-1,30),\n",
    "           title = colname+\"\\n\\nPredicts Low \"+colname+\" <---   ---> Predicts High \"+colname, dependence_plots=True)\n",
    "\n",
    "\n",
    "# r^2 = 0.04!!!!! Almost like other people worked to try to make films profitable!\n",
    "### wait - we got .38 on earlier run with *worse data*\n",
    "# difference? -> much better now we normalised the target\n",
    "\n",
    "\n",
    "# release_year -> older movies, more profitable, less competition\n",
    "# low profitability => not many votes (less so if film recent)\n",
    "# belongs to collection => prob profitable! (less so if film recent)\n",
    "# most of the gross domestic = prob not profitable!\n",
    "# short movies - more likely to be profitable (more so for older movies) - really sharp cutoff at 100mins!\n",
    "# international releases - *very linear* (negative) relation between domestic gross fraction and profitability (less so older movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-991b2fc96854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "\n",
    "def shap_outputs(shap_values, train, target_var, output_subfolder,\n",
    "                 dependence_plots = False, threshold = .1, min_features = 30,\n",
    "                 title=None):\n",
    "\n",
    "    #################################\n",
    "#     threshold = .1\n",
    "#     min_features = 30\n",
    "    global_shap_vals = np.abs(shap_values).mean(0)#[::-1]\n",
    "    n_top_features = max( sum(global_shap_vals[np.argsort(global_shap_vals)]>=threshold),\n",
    "                          min_features )\n",
    "#     if n_top_features <min_features:\n",
    "#         n_top_features = min_features\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    inds = np.argsort(global_shap_vals)[-n_top_features:]\n",
    "\n",
    "    y_pos = np.arange(n_top_features)\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.title(target_var);\n",
    "    plt.barh(y_pos, global_shap_vals[inds], color=\"#1E88E5\")\n",
    "    plt.yticks(y_pos, train.columns[inds])\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.xlabel(\"mean SHAP value magnitude (change in log odds)\")\n",
    "    plt.gcf().set_size_inches(6, 4.5)\n",
    "\n",
    "    plt.savefig( output_subfolder + \"mean_impact\" + \".png\", bbox_inches='tight' )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ####################\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    if title is None:\n",
    "        fig.suptitle(target_var);\n",
    "    else:\n",
    "        fig.suptitle(title);\n",
    "        \n",
    "    shap.summary_plot( shap_values, train, max_display=n_top_features, plot_type='dot' );\n",
    "    shap_problem = np.isnan(np.abs(shap_values).mean(0)).any()\n",
    "    if shap_problem:\n",
    "        summary_text = \"summary_plot(approx)\"\n",
    "    else:\n",
    "        summary_text = \"summary_plot\"\n",
    "    \n",
    "    fig.savefig( output_subfolder + summary_text + \".png\", bbox_inches='tight' )\n",
    "    \n",
    "        ##################\n",
    "    if dependence_plots:\n",
    "        count = 0\n",
    "        for name in train.columns[inds[::-1]]:\n",
    "            fig = plt.figure(figsize = (16,10))\n",
    "            fig.suptitle(target_var);\n",
    "            shap.dependence_plot(name, shap_values, train)\n",
    "            clean_filename(name)\n",
    "            fig.savefig(output_subfolder + \"featureNo \"+str(count) + \" \" + clean_filename(name) + \".png\", bbox_inches='tight')\n",
    "            count = count + 1\n",
    "            \n",
    "def get_non_overfit_settings( train, target, alg, seed, early_stoppping_fraction, test_size, eval_metric, verbose = True,\n",
    "                              sample_weights = None ):\n",
    "\n",
    "    if sample_weights is not None:\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split( pd.concat( [train,sample_weights], axis=1 ),\n",
    "                                                             target, test_size=test_size,\n",
    "                                                             random_state=seed, stratify=pd.qcut( pd.Series( target ),\n",
    "                                                                                                  q=10,\n",
    "                                                                                                  duplicates = 'drop',\n",
    "                                                                                                ).cat.codes )\n",
    "\n",
    "        eval_set = [(X_test, y_test)]\n",
    "\n",
    "        sample_weight = X_train[weight_var].values\n",
    "        sample_weight_eval_set = X_test[weight_var].values\n",
    "        X_train.drop(weight_var, axis=1, inplace=True)\n",
    "        X_test.drop(weight_var, axis=1, inplace=True)\n",
    "\n",
    "        alg.fit(X_train, y_train, eval_metric=eval_metric, \n",
    "                early_stopping_rounds = alg.get_params()['n_estimators']*early_stoppping_fraction,\n",
    "                eval_set=eval_set, verbose=True, sample_weight = sample_weight)\n",
    "        \n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split( train,\n",
    "                                                             target, test_size=test_size,\n",
    "                                                             random_state=seed, stratify=pd.qcut( pd.Series( target ),\n",
    "                                                                                                  q=10,\n",
    "                                                                                                  duplicates = 'drop',\n",
    "                                                                                                ).cat.codes )\n",
    "          \n",
    "            \n",
    "\n",
    "        eval_set = [(X_test, y_test)]\n",
    "\n",
    "        alg.fit(X_train, y_train, eval_metric=eval_metric, \n",
    "                early_stopping_rounds = alg.get_params()['n_estimators']*early_stoppping_fraction,\n",
    "                eval_set=eval_set, verbose=True )        \n",
    "        \n",
    "\n",
    "    # make predictions for test data\n",
    "    predictions = alg.predict(X_test)\n",
    "\n",
    "    # evaluate predictions\n",
    "    MSE = mean_squared_error(y_test, predictions)\n",
    "    MAE = mean_absolute_error(y_test, predictions)\n",
    "    EV = explained_variance_score(y_test, predictions)\n",
    "    R2 = r2_score(y_test, predictions)\n",
    "\n",
    "    print(\"MSE: %.2f, MAE: %.2f, EV: %.2f, R2: %.2f\" % (MSE, MAE, EV, R2) )\n",
    "    alg.set_params(n_estimators=alg.best_iteration)            \n",
    "    \n",
    "def xgboost_run(title, subdir=None, min_features=30, dependence_plots=False ):\n",
    "    # for target_var,base_var in zip(var_list,base_list):\n",
    "    treatment_subfolder = create_subdir(BES_output_folder,\"xgb_census\"+Treatment)\n",
    "\n",
    "    for target_var in var_list:\n",
    "        if sample_wts:\n",
    "            wave_no = get_wave_no( target_var )\n",
    "            weight_var = num_to_weight[wave_no]    \n",
    "            print( target_var, wave_no )\n",
    "\n",
    "        target = create_target(target_var)\n",
    "        mask   = target.notnull()\n",
    "        if optional_mask & sample_wts:\n",
    "            mask = mask&optional_mask_fn(wave_no)\n",
    "        else:\n",
    "            mask = mask&optional_mask_fn()\n",
    "        target = target[mask]\n",
    "\n",
    "        if sum(mask) < minimum_sample:\n",
    "            continue\n",
    "\n",
    "        train = create_train(drop_other_waves)\n",
    "\n",
    "        if subdir is None:\n",
    "            output_subfolder = create_subdir(treatment_subfolder,target_var)\n",
    "        else:\n",
    "            output_subfolder = create_subdir(treatment_subfolder,subdir)\n",
    "\n",
    "        if sample_wts:\n",
    "            sample_weights = weights[weight_var][mask]\n",
    "            print(\"missing vals in sample weights: \"+ str( sample_weights.isnull().sum() ) )\n",
    "            sample_weights = sample_weights.fillna(sample_weights.median())\n",
    "        else:\n",
    "            sample_weights = None\n",
    "    #         get_non_overfit_settings( train, target, alg, seed, early_stoppping_fraction, test_size, sample_weights )\n",
    "    #         # fit to full dataset at non-overfitting level\n",
    "    #         alg.fit(train, target, verbose = True, sample_weight = sample_weights)        \n",
    "    #     else:\n",
    "\n",
    "        get_non_overfit_settings( train, target, alg, seed, early_stoppping_fraction, test_size, eval_metric, verbose = True,\n",
    "                                  sample_weights=sample_weights )\n",
    "        # fit to full dataset at non-overfitting level\n",
    "        alg.fit(train, target, verbose = True, sample_weight = sample_weights)\n",
    "\n",
    "\n",
    "    #################\n",
    "\n",
    "        explainer = shap.TreeExplainer(alg)\n",
    "        shap_values = explainer.shap_values(train)\n",
    "        \n",
    "#         shap_values = shap.TreeExplainer(alg).shap_values(train);\n",
    "\n",
    "        shap_problem = np.isnan(np.abs(shap_values).mean(0)).any()\n",
    "        if shap_problem:\n",
    "            print(\"hit problem!\")\n",
    "            shap_values = shap.TreeExplainer(alg).shap_values(train, approximate=True);\n",
    "\n",
    "        shap_outputs(shap_values, train, target_var, output_subfolder, threshold = .1,\n",
    "                     min_features = min_features, title=title,\n",
    "                     dependence_plots=dependence_plots)\n",
    "        \n",
    "    return (explainer, shap_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optional_mask = False\n",
    "sample_wts = False\n",
    "drop_other_waves = False\n",
    "\n",
    "\n",
    "# Leavers only\n",
    "def optional_mask_fn(wave=[]):\n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "def create_train(drop_other_waves):\n",
    "    keep_list = df.columns\n",
    "    \n",
    "    if drop_other_waves:\n",
    "        # drop variables from other waves\n",
    "        other_waves = get_other_wave_pattern(wave_no, max_wave, num_to_wave)\n",
    "        keep_list = [x for x in keep_list if not re.search( other_waves, x )]\n",
    "        \n",
    "    # drop key variables\n",
    "    keep_list = [x for x in keep_list if not any([var_stub in x for var_stub in var_stub_list])] \n",
    "    \n",
    "    return df[keep_list]\n",
    "\n",
    "\n",
    "def create_target(target_var):\n",
    "    \n",
    "    return df[target_var]\n",
    "\n",
    "objective = 'reg:linear'\n",
    "eval_metric = 'rmse'\n",
    "\n",
    "seed = 27\n",
    "test_size = 0.33\n",
    "minimum_sample = 100\n",
    "early_stoppping_fraction = .1\n",
    "\n",
    "alg = XGBRegressor(\n",
    " learning_rate =0.05,\n",
    " n_estimators= 508,\n",
    " max_depth=6,\n",
    " min_child_weight=6,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.6,\n",
    " colsample_bylevel=.85,\n",
    " objective= objective,\n",
    " scale_pos_weight=1.0,\n",
    " reg_alpha=5e-05,\n",
    " reg_lambda=1,\n",
    " njobs=3,\n",
    " seed=seed**2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
